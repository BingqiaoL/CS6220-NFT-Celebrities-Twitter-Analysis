{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input NFT collection names and their floor prices, market volume\n",
    "def nft_market_data(nft_name):\n",
    "    df1 = pd.read_csv(nft_name +'_floor_price.csv')\n",
    "    df2 = pd.read_csv(nft_name +'_market_volume.csv')\n",
    "    market_volume = df2['market_volume'].tolist()\n",
    "    floor_price = df1['floor_price'].tolist()\n",
    "    \n",
    "    floor_price.pop()\n",
    "    market_volume.pop()\n",
    "    return floor_price, market_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def nft_vadar_sentiment_score(nft_name):\n",
    "    # Read in the vader folder and get all \n",
    "    nft_vadar_sentiment_score = []\n",
    "    start_date = '2022-08-01'\n",
    "    end_date = '2023-02-27'\n",
    "    date_range = pd.date_range(start_date, end_date)\n",
    "    \n",
    "    for date in date_range:\n",
    "        date_str = date.strftime('%Y-%m-%d')\n",
    "        csv_file = 'vader/' + 'vader' + '_' + date_str + '.csv'\n",
    "        # filter the Text column and get all the indices\n",
    "        df = pd.read_csv(csv_file)\n",
    "        # keep only rows whose 'Text' has the nft_name\n",
    "        df = df[df['Text'].str.contains(nft_name)]\n",
    "        # if there is no row with the nft_name, then the vader score is 0\n",
    "        if df.empty:\n",
    "            nft_vadar_sentiment_score.append(0)\n",
    "            continue\n",
    "        # get the vader column's compound score\n",
    "        vader_col = df['vader']\n",
    "        # extract the compound score\n",
    "        # Extract the compound number from each row of the 'vader' column\n",
    "        compound_list = [eval(vader)['compound'] for vader in vader_col]\n",
    "        # calculate the average compound score\n",
    "        vader_avg = np.mean(compound_list)\n",
    "        nft_vadar_sentiment_score.append(vader_avg)\n",
    "    return nft_vadar_sentiment_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the function\n",
    "azuki_sentiment_scores = nft_vadar_sentiment_score('azuki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.20656249999999998, 0.765, 0.2202, -0.29569999999999996, 0.0371, 0.40519999999999995, 0.37649999999999995, 0.4404, -0.0258, 0.6065999999999999, 0.49888000000000005, 0.0, 0.0, -0.3612, 0.15766, 0.07587142857142858, 0.660225, 0.346, 0.2171833333333333, 0.14965, 0, 0.6369, -0.296, 0.5137333333333333, 0.19191428571428573, 0.29957500000000004, 0.048725000000000004, 0, 0.0, -0.20095, -0.3306, 0.031657142857142856, 0.1752166666666667, 0.0, 0.5189, 0.11011666666666665, 0.0, -0.8225, 0.079, 0.0772, 0.4215, 0.26353333333333334, 0.167625, 0.0, 0.229075, 0.12688000000000002, 0.33125, -0.32926666666666665, 0.421, 0.10366250000000002, 0.0277, -0.2352, 0.07876249999999999, 0.025733333333333334, 0.0, 0.45690000000000003, -0.0086, 0.09658333333333334, 0.20230909090909088, 0.0772, -0.045936842105263165, -0.20380000000000004, 0.2294, 0.4141333333333333, 0.2587, 0.5102, -0.015849999999999975, 0.28501999999999994, 0.5445666666666668, -0.0172, 0.21715000000000004, 0.2454555555555556, 0.14702727272727276, 0.22785, -0.09676666666666665, 0.16763333333333333, -0.0258, -0.04649166666666665, 0.15343333333333334, 0.0683, 0.014274999999999996, 0.1829375, 0.036988888888888884, 0.25915454545454547, 0.03685000000000001, -0.0985, 0.18906, 0.5704750000000001, 0.13327499999999998, 0.06743333333333333, 0.0516, 0.11120000000000003, 0.13257, -0.0172, 0.04321666666666668, 0.20782857142857142, 0.066575, -0.1953, 0.56775, 0.3182, -0.058499999999999996, 0, 0.35902, -0.025099999999999973, 0.0, 0.26654, 0.023425, 0.0, 0.09408999999999998, 0.18032857142857145, 0.10372499999999998, 0.0, 0, 0.05733999999999999, 0.367, 0, -0.11093333333333333, 0.5106, 0.29357500000000003, -0.106875, 0.286325, 0, 0.39870000000000005, 0.0, 0.0, 0.16915000000000002, 0.10115, 0.0, 0.2717583333333334, 0.2543666666666667, 0.25170000000000003, 0.20354999999999998, 0.0386, -0.3051333333333333, 0.2775, -0.1366, -0.019000000000000003, 0.3612, 0.4767, 0.01880000000000001, 0.23775000000000002, 0.2888, -0.2787, 0.6007, 0.010985714285714281, 0.6855666666666668, 0.11869999999999999, 0.17472, -0.02370000000000001, 0.041242857142857144, 0.050575, 0.11688333333333334, 0.02717500000000002, -0.06408, 0.5574, 0.3904166666666667, 0.13740833333333335, 0.20523999999999995, 0.15175, -0.026749999999999996, 0.21639999999999998, 0.2543666666666667, 0.22662857142857143, 0.3297, -0.2263, 0.37338, 0.6369, 0.07428571428571429, 0.1366, 0.36100000000000004, 0.10877142857142859, 0.08747142857142857, 0.1027, 0.0, 0.0, 0.17027142857142857, 0.16886666666666664, 0.017579999999999995, 0.054266666666666664, 0.1331642857142857, 0.0, 0.35645000000000004, 0.3062, 0.1495, 0.13262000000000002, 0.0, 0.28750000000000003, 0.44094999999999995, 0.23629999999999998, -0.2553, 0.0, 0.21795000000000003, 0.1909, 0.4588, 0.5746, 0, 0.22047499999999998, 0.28752000000000005, 0.3758666666666666, 0.10730000000000003, 0.0, 0.13473333333333334, 0, 0.16556666666666667, 0.1779, 0.22542500000000004, -0.20095, -0.24349999999999997, 0.3182, 0.12765, 0.23763333333333336]\n"
     ]
    }
   ],
   "source": [
    "# print azuki_sentiment_scores\n",
    "print(azuki_sentiment_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the sentiment scores\n",
    "# fill the missing values with 0\n",
    "azuki_sentiment_scores = [0 if np.isnan(x) else x for x in azuki_sentiment_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def nft_tweets_textual_embeddings(nft_name, embedding_name):\n",
    "    average_embeddings = []\n",
    "\n",
    "    start_date = '2022-08-01'\n",
    "    end_date = '2023-02-27'\n",
    "    date_range = pd.date_range(start_date, end_date)\n",
    "    sentiment_score = nft_vadar_sentiment_score(nft_name)\n",
    "    for date in date_range:\n",
    "        date_str = date.strftime(\"%Y-%m-%d\")\n",
    "        csv_file = os.path.join(f\"{date_str}.csv\")\n",
    "        embeddings = []\n",
    "        if os.path.exists(csv_file):\n",
    "            df = pd.read_csv(csv_file)\n",
    "            keyword_indices = df[df['Text'].str.contains(nft_name, case=False)].index\n",
    "\n",
    "            daily_embedding_folder = os.path.join(f\"/home/qian/qian/CS6220-NFT-Celebrities-Twitter-Analysis-main/{embedding_name}/{date_str}\")\n",
    "            for index in keyword_indices:\n",
    "                index_file = os.path.join(f\"{daily_embedding_folder}/{index}.npy\")\n",
    "                if os.path.exists(index_file):\n",
    "                    # keep only the first 768 dimensions\n",
    "                    embedding = np.load(index_file)[..., :768]\n",
    "                    embedding_dim = embedding.shape[0]\n",
    "                    embeddings.append(embedding)\n",
    "\n",
    "            if len(embeddings) > 0:\n",
    "                average_bert_embeddings = np.mean(embeddings, axis=0)\n",
    "                # then append vader sentiment score the average bert embeddings, \n",
    "                average_bert_embeddings = np.append(average_bert_embeddings, )\n",
    "            else:\n",
    "                average_bert_embeddings = np.zeros((768,))\n",
    "            average_embeddings.append(average_bert_embeddings)\n",
    "    return average_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test:nft_market_data('azuki')\n",
    "azuki_price, azuki_volume = nft_market_data('azuki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list to tensor\n",
    "azuki_price_tensor = torch.tensor(azuki_price, dtype=torch.float32)\n",
    "azuki_volume_tensor = torch.tensor(azuki_volume, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get azuki textual embeddings and convert to tensor\n",
    "azuki_embeddings = nft_tweets_textual_embeddings('azuki', 'bert_embeddings')\n",
    "azuki_embeddings_tensor = torch.tensor(azuki_embeddings, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_846733/3839738689.py:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /home/conda/feedstock_root/build_artifacts/pytorch-recipe_1660070785140/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  bayc_embeddings_tensor = torch.tensor(bayc_embeddings, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# get bayc data and convert to tensor\n",
    "bayc_price, bayc_volume = nft_market_data('bayc')\n",
    "bayc_price_tensor = torch.tensor(bayc_price, dtype=torch.float32)\n",
    "bayc_volume_tensor = torch.tensor(bayc_volume, dtype=torch.float32)\n",
    "bayc_embeddings = nft_tweets_textual_embeddings('bayc', 'bert_embeddings')\n",
    "bayc_embeddings_tensor = torch.tensor(bayc_embeddings, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mayc data and convert to tensor\n",
    "mayc_price, mayc_volume = nft_market_data('mayc')\n",
    "mayc_price_tensor = torch.tensor(mayc_price, dtype=torch.float32)\n",
    "mayc_volume_tensor = torch.tensor(mayc_volume, dtype=torch.float32)\n",
    "mayc_embeddings = nft_tweets_textual_embeddings('mayc', 'bert_embeddings')\n",
    "mayc_embeddings_tensor = torch.tensor(mayc_embeddings, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2123, -0.0152,  0.0763,  ..., -0.3050,  0.2155,  0.3054],\n",
       "        [-0.1550, -0.0228,  0.1331,  ..., -0.2174,  0.2856,  0.3664],\n",
       "        [-0.1934,  0.0528,  0.0591,  ..., -0.2550,  0.2446,  0.3307],\n",
       "        [-0.2766, -0.0902,  0.1316,  ..., -0.3048,  0.2624,  0.4160],\n",
       "        [-0.1113,  0.0855,  0.2011,  ..., -0.2698,  0.2867,  0.2766]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print mayc data\n",
    "mayc_embeddings_tensor[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# otherdeed data and convert to tensor\n",
    "otherdeed_price, otherdeed_volume = nft_market_data('otherdeed_for_otherside')\n",
    "otherdeed_price_tensor = torch.tensor(otherdeed_price, dtype=torch.float32)\n",
    "otherdeed_volume_tensor = torch.tensor(otherdeed_volume, dtype=torch.float32)\n",
    "otherdeed_embeddings = nft_tweets_textual_embeddings('otherdeed', 'bert_embeddings')\n",
    "otherdeed_embeddings_tensor = torch.tensor(otherdeed_embeddings, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clonex data and convert to tensor\n",
    "clonex_price, clonex_volume = nft_market_data('clonex')\n",
    "clonex_price_tensor = torch.tensor(clonex_price, dtype=torch.float32)\n",
    "clonex_volume_tensor = torch.tensor(clonex_volume, dtype=torch.float32)\n",
    "clonex_embeddings = nft_tweets_textual_embeddings('clonex', 'bert_embeddings')\n",
    "clonex_embeddings_tensor = torch.tensor(clonex_embeddings, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'azuki_embeddings_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/qian/qian/CS6220-NFT-Celebrities-Twitter-Analysis-main/LSTM_textual_features.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bxtra3090.d2.comp.nus.edu.sg/home/qian/qian/CS6220-NFT-Celebrities-Twitter-Analysis-main/LSTM_textual_features.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# print azuki_embeddings_tensor last 5 rows\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bxtra3090.d2.comp.nus.edu.sg/home/qian/qian/CS6220-NFT-Celebrities-Twitter-Analysis-main/LSTM_textual_features.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(azuki_embeddings_tensor[:\u001b[39m5\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'azuki_embeddings_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "# print azuki_embeddings_tensor last 5 rows\n",
    "print(azuki_embeddings_tensor[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 0.0640\n",
      "Epoch [2/200], Loss: 0.0404\n",
      "Epoch [3/200], Loss: 0.1220\n",
      "Epoch [4/200], Loss: 0.0459\n",
      "Epoch [5/200], Loss: 0.1161\n",
      "Epoch [6/200], Loss: 0.1043\n",
      "Epoch [7/200], Loss: 0.0914\n",
      "Epoch [8/200], Loss: 0.0418\n",
      "Epoch [9/200], Loss: 0.0516\n",
      "Epoch [10/200], Loss: 0.0708\n",
      "Epoch [11/200], Loss: 0.0927\n",
      "Epoch [12/200], Loss: 0.0717\n",
      "Epoch [13/200], Loss: 0.0477\n",
      "Epoch [14/200], Loss: 0.0747\n",
      "Epoch [15/200], Loss: 0.0739\n",
      "Epoch [16/200], Loss: 0.0773\n",
      "Epoch [17/200], Loss: 0.0482\n",
      "Epoch [18/200], Loss: 0.1058\n",
      "Epoch [19/200], Loss: 0.0830\n",
      "Epoch [20/200], Loss: 0.0525\n",
      "Epoch [21/200], Loss: 0.0994\n",
      "Early stopping at epoch 22\n",
      "Mean Squared Error: 4.86\n",
      "Root Mean Squared Error: 2.2052378161631867\n"
     ]
    }
   ],
   "source": [
    "# Train LSTM model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "\n",
    "# Prepare dataset\n",
    "X = clonex_embeddings_tensor  # Shape: (number_of_dates, 768)\n",
    "y = clonex_price_tensor # Shape: (number_of_dates, 1)\n",
    "\n",
    "# Reshape the data before normalization\n",
    "X_numpy = X.numpy()\n",
    "y_numpy = y.numpy().reshape(-1, 1)\n",
    "\n",
    "# Split the dataset into training and test sets, use first 80% of the data for training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_numpy, y_numpy, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Normalize the input data\n",
    "scaler_X = MinMaxScaler()\n",
    "X_train_normalized = scaler_X.fit_transform(X_train)\n",
    "X_test_normalized = scaler_X.transform(X_test)\n",
    "\n",
    "scaler_y = MinMaxScaler()\n",
    "y_train_normalized = scaler_y.fit_transform(y_train)\n",
    "\n",
    "# Convert the normalized data back to tensors\n",
    "X_train = torch.tensor(X_train_normalized, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test_normalized, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train_normalized, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define the LSTM model architecture\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_size = 768\n",
    "hidden_size = 32\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "drop_prob = 0.1\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size, drop_prob)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model with early stopping\n",
    "num_epochs = 200\n",
    "early_stop = 20 # Number of epochs to wait before early stopping\n",
    "best_loss = float('inf')\n",
    "best_epoch = 0\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (embeddings, targets) in enumerate(train_loader):\n",
    "        embeddings = embeddings.unsqueeze(1)  # Shape: (batch_size, 1, 770)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(embeddings)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if loss.item() < best_loss:\n",
    "        best_loss = loss.item()\n",
    "        best_epoch = epoch\n",
    "        best_model = model.state_dict()\n",
    "    elif epoch - best_epoch >= early_stop:\n",
    "        print(f'Early stopping at epoch {epoch+1}')\n",
    "        break\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    X_test = X_test.unsqueeze(1)  # Shape: (test_size, 1, 770)\n",
    "    y_pred_normalized = model(X_test)\n",
    "\n",
    "    # Inverse transform the normalized predictions to get the unnormalized predictions\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_normalized.numpy())\n",
    "\n",
    "# Calculate the mean squared error or any other performance metric\n",
    "mse = criterion(torch.tensor(y_pred), y_test)\n",
    "# keep 2 decimal places\n",
    "# Round mean squared error to two decimal places\n",
    "mse_rounded = torch.round(mse * 100) / 100\n",
    "\n",
    "# Print mean squared error with two decimal places\n",
    "print(f\"Mean Squared Error: {mse_rounded:.2f}\")\n",
    "\n",
    "rmse = math.sqrt(mse.item())\n",
    "print('Root Mean Squared Error:', rmse)\n",
    "\n",
    "# Add some code to document the results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
