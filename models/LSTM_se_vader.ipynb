{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qian/miniconda3/envs/torch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input NFT collection names and their floor prices, market volume\n",
    "def nft_market_data(nft_name):\n",
    "    df1 = pd.read_csv(nft_name +'_floor_price.csv')\n",
    "    df2 = pd.read_csv(nft_name +'_market_volume.csv')\n",
    "    market_volume = df2['market_volume'].tolist()\n",
    "    floor_price = df1['floor_price'].tolist()\n",
    "    \n",
    "    floor_price.pop()\n",
    "    market_volume.pop()\n",
    "    return floor_price, market_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def nft_vadar_sentiment_score(nft_name):\n",
    "    # Read in the vader folder and get all \n",
    "    nft_vadar_sentiment_score = []\n",
    "    start_date = '2022-08-01'\n",
    "    end_date = '2023-02-27'\n",
    "    date_range = pd.date_range(start_date, end_date)\n",
    "    \n",
    "    for date in date_range:\n",
    "        date_str = date.strftime('%Y-%m-%d')\n",
    "        csv_file = 'vader/' + 'vader' + '_' + date_str + '.csv'\n",
    "        # filter the Text column and get all the indices\n",
    "        df = pd.read_csv(csv_file)\n",
    "        # keep only rows whose 'Text' has the nft_name\n",
    "        df = df[df['Text'].str.contains(nft_name)]\n",
    "        # if there is no row with the nft_name, then the vader score is 0\n",
    "        if df.empty:\n",
    "            nft_vadar_sentiment_score.append(0)\n",
    "            continue\n",
    "        # get the vader column's compound score\n",
    "        vader_col = df['vader']\n",
    "        # extract the compound score\n",
    "        # Extract the compound number from each row of the 'vader' column\n",
    "        compound_list = [eval(vader)['compound'] for vader in vader_col]\n",
    "        # # calculate the average compound score\n",
    "        vader_avg = np.mean(compound_list)\n",
    "        nft_vadar_sentiment_score.append(vader_avg)\n",
    "    return nft_vadar_sentiment_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the function\n",
    "azuki_sentiment_scores = nft_vadar_sentiment_score('azuki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test clonex sentiment score\n",
    "clonex_sentiment_scores = nft_vadar_sentiment_score('clonex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.20656249999999998, 0.765, 0.2202, -0.29569999999999996, 0.0371, 0.40519999999999995, 0.37649999999999995, 0.4404, -0.0258, 0.6065999999999999, 0.49888000000000005, 0.0, 0.0, -0.3612, 0.15766, 0.07587142857142858, 0.660225, 0.346, 0.2171833333333333, 0.14965, 0, 0.6369, -0.296, 0.5137333333333333, 0.19191428571428573, 0.29957500000000004, 0.048725000000000004, 0, 0.0, -0.20095, -0.3306, 0.031657142857142856, 0.1752166666666667, 0.0, 0.5189, 0.11011666666666665, 0.0, -0.8225, 0.079, 0.0772, 0.4215, 0.26353333333333334, 0.167625, 0.0, 0.229075, 0.12688000000000002, 0.33125, -0.32926666666666665, 0.421, 0.10366250000000002, 0.0277, -0.2352, 0.07876249999999999, 0.025733333333333334, 0.0, 0.45690000000000003, -0.0086, 0.09658333333333334, 0.20230909090909088, 0.0772, -0.045936842105263165, -0.20380000000000004, 0.2294, 0.4141333333333333, 0.2587, 0.5102, -0.015849999999999975, 0.28501999999999994, 0.5445666666666668, -0.0172, 0.21715000000000004, 0.2454555555555556, 0.14702727272727276, 0.22785, -0.09676666666666665, 0.16763333333333333, -0.0258, -0.04649166666666665, 0.15343333333333334, 0.0683, 0.014274999999999996, 0.1829375, 0.036988888888888884, 0.25915454545454547, 0.03685000000000001, -0.0985, 0.18906, 0.5704750000000001, 0.13327499999999998, 0.06743333333333333, 0.0516, 0.11120000000000003, 0.13257, -0.0172, 0.04321666666666668, 0.20782857142857142, 0.066575, -0.1953, 0.56775, 0.3182, -0.058499999999999996, 0, 0.35902, -0.025099999999999973, 0.0, 0.26654, 0.023425, 0.0, 0.09408999999999998, 0.18032857142857145, 0.10372499999999998, 0.0, 0, 0.05733999999999999, 0.367, 0, -0.11093333333333333, 0.5106, 0.29357500000000003, -0.106875, 0.286325, 0, 0.39870000000000005, 0.0, 0.0, 0.16915000000000002, 0.10115, 0.0, 0.2717583333333334, 0.2543666666666667, 0.25170000000000003, 0.20354999999999998, 0.0386, -0.3051333333333333, 0.2775, -0.1366, -0.019000000000000003, 0.3612, 0.4767, 0.01880000000000001, 0.23775000000000002, 0.2888, -0.2787, 0.6007, 0.010985714285714281, 0.6855666666666668, 0.11869999999999999, 0.17472, -0.02370000000000001, 0.041242857142857144, 0.050575, 0.11688333333333334, 0.02717500000000002, -0.06408, 0.5574, 0.3904166666666667, 0.13740833333333335, 0.20523999999999995, 0.15175, -0.026749999999999996, 0.21639999999999998, 0.2543666666666667, 0.22662857142857143, 0.3297, -0.2263, 0.37338, 0.6369, 0.07428571428571429, 0.1366, 0.36100000000000004, 0.10877142857142859, 0.08747142857142857, 0.1027, 0.0, 0.0, 0.17027142857142857, 0.16886666666666664, 0.017579999999999995, 0.054266666666666664, 0.1331642857142857, 0.0, 0.35645000000000004, 0.3062, 0.1495, 0.13262000000000002, 0.0, 0.28750000000000003, 0.44094999999999995, 0.23629999999999998, -0.2553, 0.0, 0.21795000000000003, 0.1909, 0.4588, 0.5746, 0, 0.22047499999999998, 0.28752000000000005, 0.3758666666666666, 0.10730000000000003, 0.0, 0.13473333333333334, 0, 0.16556666666666667, 0.1779, 0.22542500000000004, -0.20095, -0.24349999999999997, 0.3182, 0.12765, 0.23763333333333336]\n"
     ]
    }
   ],
   "source": [
    "# print the sentiment score\n",
    "print(azuki_sentiment_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1806, 0, 0, 0.18116666666666667, 0.42819999999999997, 0.9118, 0.0, 0, 0.0, 0.24159999999999998, 0.38849999999999996, 0.0, 0.3612, 0.0, 0.0, 0.5574, 0.3612, 0.12123333333333335, 0, 0.0, 0.4215, 0, 0.0, 0.7717, 0.3612, 0.22646666666666668, 0, 0, 0.6908, 0.3982, 0.1033923076923077, 0.06743333333333333, 0.0, 0, -0.2814, 0.6124, 0, -0.09054999999999999, 0.34552499999999997, 0.0, 0.3919, 0.3243, 0.57486, 0.08991666666666669, 0.05930000000000003, 0.3243, 0.33525, 0.603, 0, 0.7579, 0.51585, 0.31570000000000004, 0, 0.34, 0, 0, -0.1779, 0.886, 0, 0.0, 0, 0.232775, -0.34, -0.4404, 0.22895, 0.4051600000000001, 0.0, 0.0332, 0.0, 0.132, 0.0, -0.03620000000000001, 0.3612, 0.1806, 0, 0, 0, 0.296, 0, 0, 0, 0, 0.23695, 0.1468, 0.3612, 0.7003, 0, 0.4939, 0.80225, 0, -0.128, 0.6124, 0.3612, 0, 0, 0.3612, 0, 0.0, -0.1993, 0, 0.2023, 0, 0, 0.2263, 0.3182, 0, 0, 0.29295, 0.4215, 0, 0.2023, 0, 0, 0, 0, 0, 0, 0, 0.0, 0, 0.6908, 0, 0, 0, 0, 0.3612, 0, 0.0, 0.0, 0.765, 0.6808, 0.2023, 0.8442, 0, 0.2023, -0.6808, 0.0, 0, -0.09206666666666664, 0, 0, 0.2023, 0, 0, 0, 0, 0, 0, -0.5423, 0, 0, 0, 0, 0, -0.5106, 0.11649999999999999, -0.011999999999999997, 0, -0.03790000000000001, 0, 0.0, 0.0, 0.34954999999999997, 0.26089999999999997, 0.0, 0.0, 0, 0.0, 0, 0, 0, 0.08275714285714285, 0, 0.0, 0, 0, 0.2886, 0, 0.0, 0, 0.3612, 0, 0.27653333333333335, 0, 0.8658, 0, 0.3612, 0, 0, 0, 0.2023, 0.5106, -0.5423, 0.4215, 0.3825, 0, 0, 0.24503333333333333, 0, 0, 0, 0, 0, 0, 0, 0, 0.5859, 0, 0, 0.7506, 0]\n"
     ]
    }
   ],
   "source": [
    "# print clonex sentiment scores\n",
    "print(clonex_sentiment_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211\n"
     ]
    }
   ],
   "source": [
    "# print azuki_sentiment_scores\n",
    "print(len(azuki_sentiment_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(211,)\n"
     ]
    }
   ],
   "source": [
    "# print shape of azuki_sentiment_scores\n",
    "print(np.shape(azuki_sentiment_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def nft_tweets_se_vader_embeddings(nft_name, embedding_name):\n",
    "    average_embeddings = []\n",
    "\n",
    "    start_date = '2022-08-01'\n",
    "    end_date = '2023-02-27'\n",
    "    date_range = pd.date_range(start_date, end_date)\n",
    "    sentiment_scores = nft_vadar_sentiment_score(nft_name)\n",
    "    for date in date_range:\n",
    "        date_str = date.strftime(\"%Y-%m-%d\")\n",
    "        csv_file = os.path.join(f\"{date_str}.csv\")\n",
    "        embeddings = []\n",
    "        if os.path.exists(csv_file):\n",
    "            df = pd.read_csv(csv_file)\n",
    "            keyword_indices = df[df['Text'].str.contains(nft_name, case=False)].index\n",
    "\n",
    "            daily_embedding_folder = os.path.join(f\"/home/qian/qian/CS6220-NFT-Celebrities-Twitter-Analysis-main/{embedding_name}/{date_str}\")\n",
    "            for index in keyword_indices:\n",
    "                index_file = os.path.join(f\"{daily_embedding_folder}/{index}.npy\")\n",
    "                if os.path.exists(index_file):\n",
    "                    # keep only the last 2 dimensions\n",
    "                    embedding = np.load(index_file)[...,-2:]\n",
    "                    embeddings.append(embedding)\n",
    "\n",
    "            if len(embeddings) > 0:\n",
    "                average_bert_embedding = np.mean(embeddings, axis=0)\n",
    "                # append value of vader sentiment score\n",
    "                # for i in range(len(average_bert_embedding)):\n",
    "                #     average_bert_embedding = np.append(average_bert_embedding, sentiment_scores[i])\n",
    "            else:\n",
    "                average_bert_embedding = np.zeros((2,))\n",
    "                # append value of vader sentiment score\n",
    "            average_embeddings.append(average_bert_embedding)\n",
    "    for i in range(len(average_embeddings)):\n",
    "        average_embeddings[i] = np.append(average_embeddings[i], sentiment_scores[i])\n",
    "    return average_embeddings        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test azuki bert embedding\n",
    "azuki_bert_embedding = nft_tweets_se_vader_embeddings('azuki', 'bert_embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(211, 3)\n"
     ]
    }
   ],
   "source": [
    "# print the shape of azuki bert embedding\n",
    "print(np.shape(azuki_bert_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "azuki_embeddings_tensor = torch.tensor(np.vstack(azuki_bert_embedding), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list to tensor\n",
    "# test:nft_market_data('azuki')\n",
    "azuki_price, azuki_volume = nft_market_data('azuki')\n",
    "\n",
    "azuki_price_tensor = torch.tensor(azuki_price, dtype=torch.float32)\n",
    "azuki_volume_tensor = torch.tensor(azuki_volume, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bayc data and convert to tensor\n",
    "bayc_price, bayc_volume = nft_market_data('bayc')\n",
    "bayc_price_tensor = torch.tensor(bayc_price, dtype=torch.float32)\n",
    "bayc_volume_tensor = torch.tensor(bayc_volume, dtype=torch.float32)\n",
    "bayc_embeddings = nft_tweets_se_vader_embeddings('bayc', 'bert_embeddings')\n",
    "bayc_embeddings_tensor = torch.tensor(np.vstack(bayc_embeddings), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mayc data and convert to tensor\n",
    "mayc_price, mayc_volume = nft_market_data('mayc')\n",
    "mayc_price_tensor = torch.tensor(mayc_price, dtype=torch.float32)\n",
    "mayc_volume_tensor = torch.tensor(mayc_volume, dtype=torch.float32)\n",
    "mayc_embeddings = nft_tweets_se_vader_embeddings('mayc', 'bert_embeddings')\n",
    "mayc_embeddings_tensor = torch.tensor(np.vstack(mayc_embeddings), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# otherdeed data and convert to tensor\n",
    "otherdeed_price, otherdeed_volume = nft_market_data('otherdeed_for_otherside')\n",
    "otherdeed_price_tensor = torch.tensor(otherdeed_price, dtype=torch.float32)\n",
    "otherdeed_volume_tensor = torch.tensor(otherdeed_volume, dtype=torch.float32)\n",
    "otherdeed_embeddings = nft_tweets_se_vader_embeddings('otherdeed_for_otherside', 'bert_embeddings')\n",
    "otherdeed_embeddings_tensor = torch.tensor(np.vstack(otherdeed_embeddings), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clonex data and convert to tensor\n",
    "clonex_price, clonex_volume = nft_market_data('clonex')\n",
    "clonex_price_tensor = torch.tensor(clonex_price, dtype=torch.float32)\n",
    "clonex_volume_tensor = torch.tensor(clonex_volume, dtype=torch.float32)\n",
    "clonex_embeddings = nft_tweets_se_vader_embeddings('clonex', 'bert_embeddings')\n",
    "clonex_embeddings_tensor = torch.tensor(np.vstack(clonex_embeddings), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 0.0088\n",
      "Epoch [2/200], Loss: 0.0276\n",
      "Epoch [3/200], Loss: 0.0065\n",
      "Epoch [4/200], Loss: 0.0745\n",
      "Epoch [5/200], Loss: 0.0146\n",
      "Epoch [6/200], Loss: 0.0757\n",
      "Epoch [7/200], Loss: 0.0069\n",
      "Epoch [8/200], Loss: 0.0153\n",
      "Epoch [9/200], Loss: 0.0131\n",
      "Epoch [10/200], Loss: 0.0089\n",
      "Epoch [11/200], Loss: 0.0218\n",
      "Epoch [12/200], Loss: 0.0115\n",
      "Epoch [13/200], Loss: 0.0059\n",
      "Epoch [14/200], Loss: 0.0118\n",
      "Epoch [15/200], Loss: 0.0086\n",
      "Epoch [16/200], Loss: 0.0959\n",
      "Epoch [17/200], Loss: 0.0095\n",
      "Epoch [18/200], Loss: 0.0173\n",
      "Epoch [19/200], Loss: 0.1000\n",
      "Epoch [20/200], Loss: 0.0124\n",
      "Epoch [21/200], Loss: 0.0102\n",
      "Epoch [22/200], Loss: 0.0128\n",
      "Epoch [23/200], Loss: 0.0127\n",
      "Epoch [24/200], Loss: 0.0126\n",
      "Epoch [25/200], Loss: 0.0100\n",
      "Epoch [26/200], Loss: 0.0723\n",
      "Epoch [27/200], Loss: 0.0083\n",
      "Epoch [28/200], Loss: 0.0982\n",
      "Epoch [29/200], Loss: 0.0821\n",
      "Epoch [30/200], Loss: 0.0116\n",
      "Epoch [31/200], Loss: 0.0282\n",
      "Epoch [32/200], Loss: 0.0108\n",
      "Early stopping at epoch 33\n",
      "Mean Squared Error: 297946.94\n",
      "Root Mean Squared Error: 545.8451589049773\n"
     ]
    }
   ],
   "source": [
    "# Train LSTM model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "\n",
    "# Prepare dataset\n",
    "X = clonex_embeddings_tensor  # Shape: (number_of_dates, 768)\n",
    "y = clonex_volume_tensor # Shape: (number_of_dates, 1)\n",
    "\n",
    "# Reshape the data before normalization\n",
    "X_numpy = X.numpy()\n",
    "y_numpy = y.numpy().reshape(-1, 1)\n",
    "\n",
    "# Split the dataset into training and test sets, use first 80% of the data for training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_numpy, y_numpy, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Normalize the input data\n",
    "scaler_X = MinMaxScaler()\n",
    "X_train_normalized = scaler_X.fit_transform(X_train)\n",
    "X_test_normalized = scaler_X.transform(X_test)\n",
    "\n",
    "scaler_y = MinMaxScaler()\n",
    "y_train_normalized = scaler_y.fit_transform(y_train)\n",
    "\n",
    "# Convert the normalized data back to tensors\n",
    "X_train = torch.tensor(X_train_normalized, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test_normalized, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train_normalized, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define the LSTM model architecture\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_size = 3\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "drop_prob = 0.1\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size, drop_prob)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model with early stopping\n",
    "num_epochs = 200\n",
    "early_stop = 20 # Number of epochs to wait before early stopping\n",
    "best_loss = float('inf')\n",
    "best_epoch = 0\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (embeddings, targets) in enumerate(train_loader):\n",
    "        embeddings = embeddings.unsqueeze(1)  # Shape: (batch_size, 1, 770)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(embeddings)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if loss.item() < best_loss:\n",
    "        best_loss = loss.item()\n",
    "        best_epoch = epoch\n",
    "        best_model = model.state_dict()\n",
    "    elif epoch - best_epoch >= early_stop:\n",
    "        print(f'Early stopping at epoch {epoch+1}')\n",
    "        break\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    X_test = X_test.unsqueeze(1)  # Shape: (test_size, 1, 770)\n",
    "    y_pred_normalized = model(X_test)\n",
    "\n",
    "    # Inverse transform the normalized predictions to get the unnormalized predictions\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_normalized.numpy())\n",
    "\n",
    "# Calculate the mean squared error or any other performance metric\n",
    "mse = criterion(torch.tensor(y_pred), y_test)\n",
    "# keep 2 decimal places\n",
    "# Round mean squared error to two decimal places\n",
    "mse_rounded = torch.round(mse * 100) / 100\n",
    "\n",
    "# Print mean squared error with two decimal places\n",
    "print(f\"Mean Squared Error: {mse_rounded:.2f}\")\n",
    "\n",
    "rmse = math.sqrt(mse.item())\n",
    "print('Root Mean Squared Error:', rmse)\n",
    "\n",
    "# Add some code to document the results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
