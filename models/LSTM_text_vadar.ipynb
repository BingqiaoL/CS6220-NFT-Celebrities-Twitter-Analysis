{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input NFT collection names and their floor prices, market volume\n",
    "def nft_market_data(nft_name):\n",
    "    df1 = pd.read_csv(nft_name +'_floor_price.csv')\n",
    "    df2 = pd.read_csv(nft_name +'_market_volume.csv')\n",
    "    market_volume = df2['market_volume'].tolist()\n",
    "    floor_price = df1['floor_price'].tolist()\n",
    "    \n",
    "    floor_price.pop()\n",
    "    market_volume.pop()\n",
    "    return floor_price, market_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def nft_vadar_sentiment_score(nft_name):\n",
    "    # Read in the vader folder and get all \n",
    "    nft_vadar_sentiment_score = []\n",
    "    start_date = '2022-08-01'\n",
    "    end_date = '2023-02-27'\n",
    "    date_range = pd.date_range(start_date, end_date)\n",
    "    \n",
    "    for date in date_range:\n",
    "        date_str = date.strftime('%Y-%m-%d')\n",
    "        csv_file = 'vader/' + 'vader' + '_' + date_str + '.csv'\n",
    "        # filter the Text column and get all the indices\n",
    "        df = pd.read_csv(csv_file)\n",
    "        # keep only rows whose 'Text' has the nft_name\n",
    "        df = df[df['Text'].str.contains(nft_name)]\n",
    "        # if there is no row with the nft_name, then the vader score is 0\n",
    "        if df.empty:\n",
    "            nft_vadar_sentiment_score.append(0)\n",
    "            continue\n",
    "        # get the vader column's compound score\n",
    "        vader_col = df['vader']\n",
    "        # extract the compound score\n",
    "        # Extract the compound number from each row of the 'vader' column\n",
    "        compound_list = [eval(vader)['compound'] for vader in vader_col]\n",
    "        # # calculate the average compound score\n",
    "        vader_avg = np.mean(compound_list)\n",
    "        nft_vadar_sentiment_score.append(vader_avg)\n",
    "    return nft_vadar_sentiment_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the function\n",
    "azuki_sentiment_scores = nft_vadar_sentiment_score('azuki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211\n"
     ]
    }
   ],
   "source": [
    "# print azuki_sentiment_scores\n",
    "print(len(azuki_sentiment_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(211,)\n"
     ]
    }
   ],
   "source": [
    "# print shape of azuki_sentiment_scores\n",
    "print(np.shape(azuki_sentiment_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.20656249999999998, 0.765, 0.2202, -0.29569999999999996, 0.0371, 0.40519999999999995, 0.37649999999999995, 0.4404, -0.0258, 0.6065999999999999]\n"
     ]
    }
   ],
   "source": [
    "# print first 10 elements\n",
    "print(azuki_sentiment_scores[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def nft_tweets_textual_embeddings(nft_name, embedding_name):\n",
    "    average_embeddings = []\n",
    "\n",
    "    start_date = '2022-08-01'\n",
    "    end_date = '2023-02-27'\n",
    "    date_range = pd.date_range(start_date, end_date)\n",
    "    sentiment_scores = nft_vadar_sentiment_score(nft_name)\n",
    "    for date in date_range:\n",
    "        date_str = date.strftime(\"%Y-%m-%d\")\n",
    "        csv_file = os.path.join(f\"{date_str}.csv\")\n",
    "        embeddings = []\n",
    "        if os.path.exists(csv_file):\n",
    "            df = pd.read_csv(csv_file)\n",
    "            keyword_indices = df[df['Text'].str.contains(nft_name, case=False)].index\n",
    "\n",
    "            daily_embedding_folder = os.path.join(f\"/home/qian/qian/CS6220-NFT-Celebrities-Twitter-Analysis-main/{embedding_name}/{date_str}\")\n",
    "            for index in keyword_indices:\n",
    "                index_file = os.path.join(f\"{daily_embedding_folder}/{index}.npy\")\n",
    "                if os.path.exists(index_file):\n",
    "                    # keep only the first 768 dimensions\n",
    "                    embedding = np.load(index_file)[..., :768]\n",
    "                    embeddings.append(embedding)\n",
    "\n",
    "            if len(embeddings) > 0:\n",
    "                average_bert_embedding = np.mean(embeddings, axis=0)\n",
    "                # append value of vader sentiment score\n",
    "                # for i in range(len(average_bert_embedding)):\n",
    "                #     average_bert_embedding = np.append(average_bert_embedding, sentiment_scores[i])\n",
    "            else:\n",
    "                average_bert_embedding = np.zeros((769,))\n",
    "                \n",
    "    \n",
    "            average_embeddings.append(average_bert_embedding)\n",
    "            # then for each date, we have a 768 dimension vector, now we need to append the vader sentiment score\n",
    "            \n",
    "            \n",
    "    for i in range(len(average_embeddings)):\n",
    "        average_embeddings[i] = np.append(average_embeddings[i], sentiment_scores[i])\n",
    "    return average_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test azuki bert embeddings\n",
    "azuki_bert_embeddings = nft_tweets_textual_embeddings('azuki', 'bert_embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(769,)\n"
     ]
    }
   ],
   "source": [
    "# print first 10 elements\n",
    "print(azuki_bert_embeddings[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "azuki_embeddings_tensor = torch.tensor(np.vstack(azuki_bert_embeddings), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list to tensor\n",
    "# test:nft_market_data('azuki')\n",
    "azuki_price, azuki_volume = nft_market_data('azuki')\n",
    "\n",
    "azuki_price_tensor = torch.tensor(azuki_price, dtype=torch.float32)\n",
    "azuki_volume_tensor = torch.tensor(azuki_volume, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bayc data and convert to tensor\n",
    "bayc_price, bayc_volume = nft_market_data('bayc')\n",
    "bayc_price_tensor = torch.tensor(bayc_price, dtype=torch.float32)\n",
    "bayc_volume_tensor = torch.tensor(bayc_volume, dtype=torch.float32)\n",
    "bayc_embeddings = nft_tweets_textual_embeddings('bayc', 'bert_embeddings')\n",
    "bayc_embeddings_tensor = torch.tensor(np.vstack(bayc_embeddings), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mayc data and convert to tensor\n",
    "mayc_price, mayc_volume = nft_market_data('mayc')\n",
    "mayc_price_tensor = torch.tensor(mayc_price, dtype=torch.float32)\n",
    "mayc_volume_tensor = torch.tensor(mayc_volume, dtype=torch.float32)\n",
    "mayc_embeddings = nft_tweets_textual_embeddings('mayc', 'bert_embeddings')\n",
    "mayc_embeddings_tensor = torch.tensor(np.vstack(mayc_embeddings), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# otherdeed data and convert to tensor\n",
    "otherdeed_price, otherdeed_volume = nft_market_data('otherdeed_for_otherside')\n",
    "otherdeed_price_tensor = torch.tensor(otherdeed_price, dtype=torch.float32)\n",
    "otherdeed_volume_tensor = torch.tensor(otherdeed_volume, dtype=torch.float32)\n",
    "otherdeed_embeddings = nft_tweets_textual_embeddings('otherdeed_for_otherside', 'bert_embeddings')\n",
    "otherdeed_embeddings_tensor = torch.tensor(np.vstack(otherdeed_embeddings), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 769 and the array at index 55 has size 770",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/qian/qian/CS6220-NFT-Celebrities-Twitter-Analysis-main/LSTM_text_vadar.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bxtra3090.d2.comp.nus.edu.sg/home/qian/qian/CS6220-NFT-Celebrities-Twitter-Analysis-main/LSTM_text_vadar.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m clonex_volume_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(clonex_volume, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bxtra3090.d2.comp.nus.edu.sg/home/qian/qian/CS6220-NFT-Celebrities-Twitter-Analysis-main/LSTM_text_vadar.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m clonex_embeddings \u001b[39m=\u001b[39m nft_tweets_textual_embeddings(\u001b[39m'\u001b[39m\u001b[39mclonex\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbert_embeddings\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bxtra3090.d2.comp.nus.edu.sg/home/qian/qian/CS6220-NFT-Celebrities-Twitter-Analysis-main/LSTM_text_vadar.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m clonex_embeddings_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(np\u001b[39m.\u001b[39;49mvstack(clonex_embeddings), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/numpy/core/shape_base.py:282\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(arrs, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    281\u001b[0m     arrs \u001b[39m=\u001b[39m [arrs]\n\u001b[0;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m _nx\u001b[39m.\u001b[39;49mconcatenate(arrs, \u001b[39m0\u001b[39;49m)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 769 and the array at index 55 has size 770"
     ]
    }
   ],
   "source": [
    "# clonex data and convert to tensor\n",
    "clonex_price, clonex_volume = nft_market_data('clonex')\n",
    "clonex_price_tensor = torch.tensor(clonex_price, dtype=torch.float32)\n",
    "clonex_volume_tensor = torch.tensor(clonex_volume, dtype=torch.float32)\n",
    "clonex_embeddings = nft_tweets_textual_embeddings('clonex', 'bert_embeddings')\n",
    "clonex_embeddings_tensor = torch.tensor(np.vstack(clonex_embeddings), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 0.3874\n",
      "Epoch [2/200], Loss: 0.2316\n",
      "Epoch [3/200], Loss: 0.0379\n",
      "Epoch [4/200], Loss: 0.0117\n",
      "Epoch [5/200], Loss: 0.0170\n",
      "Epoch [6/200], Loss: 0.0050\n",
      "Epoch [7/200], Loss: 0.0122\n",
      "Epoch [8/200], Loss: 0.0137\n",
      "Epoch [9/200], Loss: 0.0078\n",
      "Epoch [10/200], Loss: 0.0066\n",
      "Epoch [11/200], Loss: 0.0018\n",
      "Epoch [12/200], Loss: 0.0885\n",
      "Epoch [13/200], Loss: 0.0129\n",
      "Epoch [14/200], Loss: 0.0065\n",
      "Epoch [15/200], Loss: 0.0036\n",
      "Epoch [16/200], Loss: 0.0171\n",
      "Epoch [17/200], Loss: 0.0063\n",
      "Epoch [18/200], Loss: 0.0150\n",
      "Epoch [19/200], Loss: 0.0095\n",
      "Epoch [20/200], Loss: 0.1008\n",
      "Early stopping at epoch 21\n",
      "Mean Squared Error: 16.59\n",
      "Root Mean Squared Error: 4.0725702917356665\n"
     ]
    }
   ],
   "source": [
    "# Train LSTM model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "\n",
    "# Prepare dataset\n",
    "X = bayc_embeddings_tensor  # Shape: (number_of_dates, 769)\n",
    "y = bayc_price_tensor # Shape: (number_of_dates, 1)\n",
    "\n",
    "# Reshape the data before normalization\n",
    "X_numpy = X.numpy()\n",
    "y_numpy = y.numpy().reshape(-1, 1)\n",
    "\n",
    "# Split the dataset into training and test sets, use first 80% of the data for training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_numpy, y_numpy, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Normalize the input data\n",
    "scaler_X = MinMaxScaler()\n",
    "X_train_normalized = scaler_X.fit_transform(X_train)\n",
    "X_test_normalized = scaler_X.transform(X_test)\n",
    "\n",
    "scaler_y = MinMaxScaler()\n",
    "y_train_normalized = scaler_y.fit_transform(y_train)\n",
    "\n",
    "# Convert the normalized data back to tensors\n",
    "X_train = torch.tensor(X_train_normalized, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test_normalized, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train_normalized, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define the LSTM model architecture\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_size = 769\n",
    "hidden_size = 32\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "drop_prob = 0.2\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size, drop_prob)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model with early stopping\n",
    "num_epochs = 200\n",
    "early_stop = 10 # Number of epochs to wait before early stopping\n",
    "best_loss = float('inf')\n",
    "best_epoch = 0\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (embeddings, targets) in enumerate(train_loader):\n",
    "        embeddings = embeddings.unsqueeze(1)  # Shape: (batch_size, 1, 770)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(embeddings)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if loss.item() < best_loss:\n",
    "        best_loss = loss.item()\n",
    "        best_epoch = epoch\n",
    "        best_model = model.state_dict()\n",
    "    elif epoch - best_epoch >= early_stop:\n",
    "        print(f'Early stopping at epoch {epoch+1}')\n",
    "        break\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    X_test = X_test.unsqueeze(1)  # Shape: (test_size, 1, 770)\n",
    "    y_pred_normalized = model(X_test)\n",
    "\n",
    "    # Inverse transform the normalized predictions to get the unnormalized predictions\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_normalized.numpy())\n",
    "\n",
    "# Calculate the mean squared error or any other performance metric\n",
    "mse = criterion(torch.tensor(y_pred), y_test)\n",
    "# keep 2 decimal places\n",
    "# Round mean squared error to two decimal places\n",
    "mse_rounded = torch.round(mse * 100) / 100\n",
    "\n",
    "# Print mean squared error with two decimal places\n",
    "print(f\"Mean Squared Error: {mse_rounded:.2f}\")\n",
    "\n",
    "rmse = math.sqrt(mse.item())\n",
    "print('Root Mean Squared Error:', rmse)\n",
    "\n",
    "# Add some code to document the results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
