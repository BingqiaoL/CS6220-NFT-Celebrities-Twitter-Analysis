{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qian/miniconda3/envs/torch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input NFT collection names and their floor prices, market volume\n",
    "def nft_market_data(nft_name):\n",
    "    df1 = pd.read_csv(nft_name +'_floor_price.csv')\n",
    "    df2 = pd.read_csv(nft_name +'_market_volume.csv')\n",
    "    market_volume = df2['market_volume'].tolist()\n",
    "    floor_price = df1['floor_price'].tolist()\n",
    "    \n",
    "    floor_price.pop()\n",
    "    market_volume.pop()\n",
    "    return floor_price, market_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def nft_vadar_sentiment_score(nft_name):\n",
    "    # Read in the vader folder and get all \n",
    "    nft_vadar_sentiment_score = []\n",
    "    start_date = '2022-08-01'\n",
    "    end_date = '2023-02-27'\n",
    "    date_range = pd.date_range(start_date, end_date)\n",
    "    \n",
    "    for date in date_range:\n",
    "        date_str = date.strftime('%Y-%m-%d')\n",
    "        csv_file = 'vader/' + 'vader' + '_' + date_str + '.csv'\n",
    "        # filter the Text column and get all the indices\n",
    "        df = pd.read_csv(csv_file)\n",
    "        # keep only rows whose 'Text' has the nft_name\n",
    "        df = df[df['Text'].str.contains(nft_name)]\n",
    "        # if there is no row with the nft_name, then the vader score is 0\n",
    "        if df.empty:\n",
    "            nft_vadar_sentiment_score.append(0)\n",
    "            continue\n",
    "        # get the vader column's compound score\n",
    "        vader_col = df['vader']\n",
    "        # extract the compound score\n",
    "        # Extract the compound number from each row of the 'vader' column\n",
    "        compound_list = [eval(vader)['compound'] for vader in vader_col]\n",
    "        # calculate the average compound score\n",
    "        vader_avg = np.mean(compound_list)\n",
    "        nft_vadar_sentiment_score.append(vader_avg)\n",
    "    return nft_vadar_sentiment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def nft_tweets_all_embeddings(nft_name, embedding_name):\n",
    "    average_embeddings = []\n",
    "\n",
    "    start_date = '2022-08-01'\n",
    "    end_date = '2023-02-27'\n",
    "    date_range = pd.date_range(start_date, end_date)\n",
    "    sentiment_scores = nft_vadar_sentiment_score(nft_name)\n",
    "    for date in date_range:\n",
    "        date_str = date.strftime(\"%Y-%m-%d\")\n",
    "        csv_file = os.path.join(f\"{date_str}.csv\")\n",
    "        embeddings = []\n",
    "        if os.path.exists(csv_file):\n",
    "            df = pd.read_csv(csv_file)\n",
    "            keyword_indices = df[df['Text'].str.contains(nft_name, case=False)].index\n",
    "\n",
    "            daily_embedding_folder = os.path.join(f\"/home/qian/qian/CS6220-NFT-Celebrities-Twitter-Analysis-main/{embedding_name}/{date_str}\")\n",
    "            for index in keyword_indices:\n",
    "                index_file = os.path.join(f\"{daily_embedding_folder}/{index}.npy\")\n",
    "                if os.path.exists(index_file):\n",
    "                    # keep all dimensions of the embedding\n",
    "                    embedding = np.load(index_file)\n",
    "                    embeddings.append(embedding)\n",
    "\n",
    "            if len(embeddings) > 0:\n",
    "                average_bert_embedding = np.mean(embeddings, axis=0)\n",
    "                # append value of vader sentiment score\n",
    "                # for i in range(len(average_bert_embedding)):\n",
    "                #     average_bert_embedding = np.append(average_bert_embedding, sentiment_scores[i])\n",
    "            else:\n",
    "                average_bert_embedding = np.zeros((770,))\n",
    "                # append value of vader sentiment score\n",
    "            average_embeddings.append(average_bert_embedding)\n",
    "    for i in range(len(average_embeddings)):\n",
    "        average_embeddings[i] = np.append(average_embeddings[i], sentiment_scores[i])\n",
    "    return average_embeddings        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the function\n",
    "nft_name = 'azuki'\n",
    "floor_price, market_volume = nft_market_data(nft_name)\n",
    "vader_score = nft_vadar_sentiment_score(nft_name)\n",
    "bert_embeddings = nft_tweets_all_embeddings(nft_name, 'bert_embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test:nft_market_data('azuki')\n",
    "azuki_price, azuki_volume = nft_market_data('azuki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list to tensor\n",
    "azuki_price_tensor = torch.tensor(azuki_price, dtype=torch.float32)\n",
    "azuki_volume_tensor = torch.tensor(azuki_volume, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get azuki textual embeddings and convert to tensor\n",
    "azuki_embeddings = nft_tweets_all_embeddings('azuki', 'bert_embeddings')\n",
    "azuki_embeddings_tensor = torch.tensor(np.vstack(azuki_embeddings), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([211, 771])\n"
     ]
    }
   ],
   "source": [
    "# print shape of azuki_embeddings\n",
    "print(azuki_embeddings_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bayc data and convert to tensor\n",
    "bayc_price, bayc_volume = nft_market_data('bayc')\n",
    "bayc_price_tensor = torch.tensor(bayc_price, dtype=torch.float32)\n",
    "bayc_volume_tensor = torch.tensor(bayc_volume, dtype=torch.float32)\n",
    "bayc_embeddings = nft_tweets_all_embeddings('bayc', 'bert_embeddings')\n",
    "bayc_embeddings_tensor = torch.tensor(np.vstack(bayc_embeddings), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mayc data and convert to tensor\n",
    "mayc_price, mayc_volume = nft_market_data('mayc')\n",
    "mayc_price_tensor = torch.tensor(mayc_price, dtype=torch.float32)\n",
    "mayc_volume_tensor = torch.tensor(mayc_volume, dtype=torch.float32)\n",
    "mayc_embeddings = nft_tweets_all_embeddings('mayc', 'bert_embeddings')\n",
    "mayc_embeddings_tensor = torch.tensor(np.vstack(mayc_embeddings), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# otherdeed data and convert to tensor\n",
    "otherdeed_price, otherdeed_volume = nft_market_data('otherdeed_for_otherside')\n",
    "otherdeed_price_tensor = torch.tensor(otherdeed_price, dtype=torch.float32)\n",
    "otherdeed_volume_tensor = torch.tensor(otherdeed_volume, dtype=torch.float32)\n",
    "otherdeed_embeddings = nft_tweets_all_embeddings('otherdeed', 'bert_embeddings')\n",
    "otherdeed_embeddings_tensor = torch.tensor(np.vstack(otherdeed_embeddings), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clonex data and convert to tensor\n",
    "clonex_price, clonex_volume = nft_market_data('clonex')\n",
    "clonex_price_tensor = torch.tensor(clonex_price, dtype=torch.float32)\n",
    "clonex_volume_tensor = torch.tensor(clonex_volume, dtype=torch.float32)\n",
    "clonex_embeddings = nft_tweets_all_embeddings('clonex', 'bert_embeddings')\n",
    "clonex_embeddings_tensor = torch.tensor(np.vstack(clonex_embeddings), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 0.0087\n",
      "Epoch [2/200], Loss: 0.0068\n",
      "Epoch [3/200], Loss: 0.0723\n",
      "Epoch [4/200], Loss: 0.0249\n",
      "Epoch [5/200], Loss: 0.0089\n",
      "Epoch [6/200], Loss: 0.0119\n",
      "Epoch [7/200], Loss: 0.0060\n",
      "Epoch [8/200], Loss: 0.0121\n",
      "Epoch [9/200], Loss: 0.0123\n",
      "Epoch [10/200], Loss: 0.0204\n",
      "Epoch [11/200], Loss: 0.0156\n",
      "Epoch [12/200], Loss: 0.0067\n",
      "Epoch [13/200], Loss: 0.0196\n",
      "Epoch [14/200], Loss: 0.0764\n",
      "Epoch [15/200], Loss: 0.0089\n",
      "Epoch [16/200], Loss: 0.0103\n",
      "Early stopping at epoch 17\n",
      "Mean Squared Error: 300592.12\n",
      "Root Mean Squared Error: 548.2628247474016\n"
     ]
    }
   ],
   "source": [
    "# Train LSTM model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "\n",
    "# Prepare dataset\n",
    "X = clonex_embeddings_tensor  # Shape: (number_of_dates, 768)\n",
    "y = clonex_volume_tensor # Shape: (number_of_dates, 1)\n",
    "\n",
    "# Reshape the data before normalization\n",
    "X_numpy = X.numpy()\n",
    "y_numpy = y.numpy().reshape(-1, 1)\n",
    "\n",
    "# Split the dataset into training and test sets, use first 80% of the data for training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_numpy, y_numpy, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Normalize the input data\n",
    "scaler_X = MinMaxScaler()\n",
    "X_train_normalized = scaler_X.fit_transform(X_train)\n",
    "X_test_normalized = scaler_X.transform(X_test)\n",
    "\n",
    "scaler_y = MinMaxScaler()\n",
    "y_train_normalized = scaler_y.fit_transform(y_train)\n",
    "\n",
    "# Convert the normalized data back to tensors\n",
    "X_train = torch.tensor(X_train_normalized, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test_normalized, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train_normalized, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define the LSTM model architecture\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_size = 771\n",
    "hidden_size = 32\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "drop_prob = 0.1\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size, drop_prob)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model with early stopping\n",
    "num_epochs = 200\n",
    "early_stop = 10 # Number of epochs to wait before early stopping\n",
    "best_loss = float('inf')\n",
    "best_epoch = 0\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (embeddings, targets) in enumerate(train_loader):\n",
    "        embeddings = embeddings.unsqueeze(1)  # Shape: (batch_size, 1, 770)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(embeddings)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if loss.item() < best_loss:\n",
    "        best_loss = loss.item()\n",
    "        best_epoch = epoch\n",
    "        best_model = model.state_dict()\n",
    "    elif epoch - best_epoch >= early_stop:\n",
    "        print(f'Early stopping at epoch {epoch+1}')\n",
    "        break\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    X_test = X_test.unsqueeze(1)  # Shape: (test_size, 1, 770)\n",
    "    y_pred_normalized = model(X_test)\n",
    "\n",
    "    # Inverse transform the normalized predictions to get the unnormalized predictions\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_normalized.numpy())\n",
    "\n",
    "# Calculate the mean squared error or any other performance metric\n",
    "mse = criterion(torch.tensor(y_pred), y_test)\n",
    "# keep 2 decimal places\n",
    "# Round mean squared error to two decimal places\n",
    "mse_rounded = torch.round(mse * 100) / 100\n",
    "\n",
    "# Print mean squared error with two decimal places\n",
    "print(f\"Mean Squared Error: {mse_rounded:.2f}\")\n",
    "\n",
    "rmse = math.sqrt(mse.item())\n",
    "print('Root Mean Squared Error:', rmse)\n",
    "\n",
    "# Add some code to document the results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
