{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eff4580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4c99f2",
   "metadata": {},
   "source": [
    "# Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1712b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1511"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mypath = 'tweets_first_run'\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "len(onlyfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c55ae83e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1819"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mypath = 'tweets_second_run'\n",
    "onlyfiles2 = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "len(onlyfiles2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02ac08c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆| 1511/1511 [02:19<00:00, 10.84it/s]\n",
      " 14%|笆遺毎笆遺毎笆遺毎笆遺槙                                              | 262/1819 [00:50<03:36,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".DS_Store\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎笆| 1819/1819 [09:48<00:00,  3.09it/s]\n"
     ]
    }
   ],
   "source": [
    "tweets = pd.DataFrame()\n",
    "\n",
    "for f in tqdm(onlyfiles):\n",
    "    try:\n",
    "        tweet = pd.read_csv('tweets_first_run/{}'.format(f))\n",
    "        tweets = tweets.append(tweet)\n",
    "    except:\n",
    "        print(f)\n",
    "\n",
    "for f in tqdm(onlyfiles2):\n",
    "    try:\n",
    "        tweet = pd.read_csv('tweets_second_run/{}'.format(f))\n",
    "        tweets = tweets.append(tweet)\n",
    "    except:\n",
    "        print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b43c5098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Username</th>\n",
       "      <th>User handle</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Date of posting</th>\n",
       "      <th>Text</th>\n",
       "      <th>Retweet count</th>\n",
       "      <th>Like count</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Getgems TON NFT Marketplace</td>\n",
       "      <td>getgemsdotio</td>\n",
       "      <td>https://twitter.com/getgemsdotio/status/163025...</td>\n",
       "      <td>2023-02-27 17:19:55+00:00</td>\n",
       "      <td>@Tleubayev @LostDogsCo Who? Who? Who? 操</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Getgems TON NFT Marketplace</td>\n",
       "      <td>getgemsdotio</td>\n",
       "      <td>https://twitter.com/getgemsdotio/status/163022...</td>\n",
       "      <td>2023-02-27 15:21:22+00:00</td>\n",
       "      <td>More details about the new collection here筮ｸ十\n...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Getgems TON NFT Marketplace</td>\n",
       "      <td>getgemsdotio</td>\n",
       "      <td>https://twitter.com/getgemsdotio/status/163022...</td>\n",
       "      <td>2023-02-27 15:21:20+00:00</td>\n",
       "      <td>Remember we asked you to guess who's in the pi...</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Getgems TON NFT Marketplace</td>\n",
       "      <td>getgemsdotio</td>\n",
       "      <td>https://twitter.com/getgemsdotio/status/163021...</td>\n",
       "      <td>2023-02-27 14:35:23+00:00</td>\n",
       "      <td>璃 https://t.co/frQpwixZ5s  @ston_fi had a prod...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Getgems TON NFT Marketplace</td>\n",
       "      <td>getgemsdotio</td>\n",
       "      <td>https://twitter.com/getgemsdotio/status/163021...</td>\n",
       "      <td>2023-02-27 14:35:23+00:00</td>\n",
       "      <td>璃 Fanzee @fanzeelabs token got listed on @Coin...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Username   User handle  \\\n",
       "0  Getgems TON NFT Marketplace  getgemsdotio   \n",
       "1  Getgems TON NFT Marketplace  getgemsdotio   \n",
       "2  Getgems TON NFT Marketplace  getgemsdotio   \n",
       "3  Getgems TON NFT Marketplace  getgemsdotio   \n",
       "4  Getgems TON NFT Marketplace  getgemsdotio   \n",
       "\n",
       "                                               Tweet  \\\n",
       "0  https://twitter.com/getgemsdotio/status/163025...   \n",
       "1  https://twitter.com/getgemsdotio/status/163022...   \n",
       "2  https://twitter.com/getgemsdotio/status/163022...   \n",
       "3  https://twitter.com/getgemsdotio/status/163021...   \n",
       "4  https://twitter.com/getgemsdotio/status/163021...   \n",
       "\n",
       "             Date of posting  \\\n",
       "0  2023-02-27 17:19:55+00:00   \n",
       "1  2023-02-27 15:21:22+00:00   \n",
       "2  2023-02-27 15:21:20+00:00   \n",
       "3  2023-02-27 14:35:23+00:00   \n",
       "4  2023-02-27 14:35:23+00:00   \n",
       "\n",
       "                                                Text Retweet count Like count  \\\n",
       "0            @Tleubayev @LostDogsCo Who? Who? Who? 操             0          2   \n",
       "1  More details about the new collection here筮ｸ十\n...             0          1   \n",
       "2  Remember we asked you to guess who's in the pi...             3          7   \n",
       "3  璃 https://t.co/frQpwixZ5s  @ston_fi had a prod...             0          2   \n",
       "4  璃 Fanzee @fanzeelabs token got listed on @Coin...             0          1   \n",
       "\n",
       "  Unnamed: 0  \n",
       "0        NaN  \n",
       "1        NaN  \n",
       "2        NaN  \n",
       "3        NaN  \n",
       "4        NaN  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7248640a",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_columns = ['Username', 'User handle', 'Tweet', 'Date of posting',\n",
    "       'Text', 'Retweet count', 'Like count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d9c2963",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets[select_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6adf3be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Username</th>\n",
       "      <th>User handle</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Date of posting</th>\n",
       "      <th>Text</th>\n",
       "      <th>Retweet count</th>\n",
       "      <th>Like count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Getgems TON NFT Marketplace</td>\n",
       "      <td>getgemsdotio</td>\n",
       "      <td>https://twitter.com/getgemsdotio/status/163025...</td>\n",
       "      <td>2023-02-27 17:19:55+00:00</td>\n",
       "      <td>@Tleubayev @LostDogsCo Who? Who? Who? 操</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Getgems TON NFT Marketplace</td>\n",
       "      <td>getgemsdotio</td>\n",
       "      <td>https://twitter.com/getgemsdotio/status/163022...</td>\n",
       "      <td>2023-02-27 15:21:22+00:00</td>\n",
       "      <td>More details about the new collection here筮ｸ十\n...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Getgems TON NFT Marketplace</td>\n",
       "      <td>getgemsdotio</td>\n",
       "      <td>https://twitter.com/getgemsdotio/status/163022...</td>\n",
       "      <td>2023-02-27 15:21:20+00:00</td>\n",
       "      <td>Remember we asked you to guess who's in the pi...</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Getgems TON NFT Marketplace</td>\n",
       "      <td>getgemsdotio</td>\n",
       "      <td>https://twitter.com/getgemsdotio/status/163021...</td>\n",
       "      <td>2023-02-27 14:35:23+00:00</td>\n",
       "      <td>璃 https://t.co/frQpwixZ5s  @ston_fi had a prod...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Getgems TON NFT Marketplace</td>\n",
       "      <td>getgemsdotio</td>\n",
       "      <td>https://twitter.com/getgemsdotio/status/163021...</td>\n",
       "      <td>2023-02-27 14:35:23+00:00</td>\n",
       "      <td>璃 Fanzee @fanzeelabs token got listed on @Coin...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Username   User handle  \\\n",
       "0  Getgems TON NFT Marketplace  getgemsdotio   \n",
       "1  Getgems TON NFT Marketplace  getgemsdotio   \n",
       "2  Getgems TON NFT Marketplace  getgemsdotio   \n",
       "3  Getgems TON NFT Marketplace  getgemsdotio   \n",
       "4  Getgems TON NFT Marketplace  getgemsdotio   \n",
       "\n",
       "                                               Tweet  \\\n",
       "0  https://twitter.com/getgemsdotio/status/163025...   \n",
       "1  https://twitter.com/getgemsdotio/status/163022...   \n",
       "2  https://twitter.com/getgemsdotio/status/163022...   \n",
       "3  https://twitter.com/getgemsdotio/status/163021...   \n",
       "4  https://twitter.com/getgemsdotio/status/163021...   \n",
       "\n",
       "             Date of posting  \\\n",
       "0  2023-02-27 17:19:55+00:00   \n",
       "1  2023-02-27 15:21:22+00:00   \n",
       "2  2023-02-27 15:21:20+00:00   \n",
       "3  2023-02-27 14:35:23+00:00   \n",
       "4  2023-02-27 14:35:23+00:00   \n",
       "\n",
       "                                                Text Retweet count Like count  \n",
       "0            @Tleubayev @LostDogsCo Who? Who? Who? 操             0          2  \n",
       "1  More details about the new collection here筮ｸ十\n...             0          1  \n",
       "2  Remember we asked you to guess who's in the pi...             3          7  \n",
       "3  璃 https://t.co/frQpwixZ5s  @ston_fi had a prod...             0          2  \n",
       "4  璃 Fanzee @fanzeelabs token got listed on @Coin...             0          1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = tweets.drop_duplicates()\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ea71d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2280945, 7)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "694375a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv('data/all_tweets.csv', index = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbf3752",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bac1fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/luobingqiao/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "946975ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv('data/all_tweets.csv')\n",
    "tweets = tweets.head(100)\n",
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf1536a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean tweets\n",
    "def clean_tweet(tweet):\n",
    "    # Convert to string if not a string\n",
    "    tweet = str(tweet)\n",
    "    \n",
    "    # Remove URLs\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", tweet, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove user mentions\n",
    "    tweet = re.sub(r\"@\\w+\", \"\", tweet)\n",
    "\n",
    "    # Remove special characters\n",
    "    tweet = re.sub(r\"\\W\", \" \", tweet)\n",
    "\n",
    "    # Remove extra spaces\n",
    "    tweet = re.sub(r\"\\s+\", \" \", tweet)\n",
    "\n",
    "    return tweet.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "235d76c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               @Tleubayev @LostDogsCo Who? Who? Who? 操\n",
       "1     More details about the new collection here筮ｸ十\n...\n",
       "2     Remember we asked you to guess who's in the pi...\n",
       "3     璃 https://t.co/frQpwixZ5s  @ston_fi had a prod...\n",
       "4     璃 Fanzee @fanzeelabs token got listed on @Coin...\n",
       "                            ...                        \n",
       "95                          @0xBrigandine @loomdart lol\n",
       "96              @DanishCryptoDK @MORBS15 this is sick!､ｩ\n",
       "97                @Tleubayev you can't be too careful Ыn",
       "98    gm frens! now that Jan activity with eggs is f...\n",
       "99           @Tleubayev ｫ｡･ｰ thanks for the support, fam!\n",
       "Name: Text, Length: 100, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b79f241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                           Who Who Who\n",
       "1            More details about the new collection here\n",
       "2     Remember we asked you to guess who s in the pi...\n",
       "3     had a product update The profitability of liqu...\n",
       "4     Fanzee token got listed on After joining the p...\n",
       "                            ...                        \n",
       "95                                                  lol\n",
       "96                                         this is sick\n",
       "97                             you can t be too careful\n",
       "98    gm frens now that Jan activity with eggs is fi...\n",
       "99                           thanks for the support fam\n",
       "Name: clean_text, Length: 100, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['clean_text'] = tweets['Text'].apply(clean_tweet)\n",
    "tweets['clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15174081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess tweets\n",
    "def preprocess_tweet(tweet):\n",
    "\n",
    "    # Tokenize the tweet\n",
    "    tokenizer = TweetTokenizer(preserve_case=False)\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatized_tokens = []\n",
    "    for token in tokens:\n",
    "        doc = nlp(token)\n",
    "        lemmatized_tokens.append([t.lemma_ for t in doc][0])\n",
    "\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in lemmatized_tokens]\n",
    "\n",
    "    return stemmed_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acf7e550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                    []\n",
       "1                                [detail, new, collect]\n",
       "2     [rememb, ask, guess, pictur, yep, adam, eve, d...\n",
       "3     [product, updat, profit, liquid, pool, import,...\n",
       "4     [fanze, token, get, list, join, project, team,...\n",
       "                            ...                        \n",
       "95                                                [lol]\n",
       "96                                               [sick]\n",
       "97                                               [care]\n",
       "98    [gm, fren, jan, activ, egg, finish, suggest, i...\n",
       "99                                [thank, support, fam]\n",
       "Name: tokenize_text, Length: 100, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['tokenize_text'] = tweets['clean_text'].apply(preprocess_tweet)\n",
    "tweets['tokenize_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82842e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6cfc173",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cd4e644",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There was a problem when trying to write in your cache folder (/Users/luobingqiao/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/luobingqiao/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# VADER\n",
    "def analyze_sentiment_vader(tweet):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment_score = sia.polarity_scores(' '.join(tweet))\n",
    "    return sentiment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a789d739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound...\n",
       "1     {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...\n",
       "2     {'neg': 0.087, 'neu': 0.841, 'pos': 0.071, 'co...\n",
       "3     {'neg': 0.0, 'neu': 0.734, 'pos': 0.266, 'comp...\n",
       "4     {'neg': 0.0, 'neu': 0.657, 'pos': 0.343, 'comp...\n",
       "                            ...                        \n",
       "95    {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound...\n",
       "96    {'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound...\n",
       "97    {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound...\n",
       "98    {'neg': 0.0, 'neu': 0.679, 'pos': 0.321, 'comp...\n",
       "99    {'neg': 0.0, 'neu': 0.161, 'pos': 0.839, 'comp...\n",
       "Name: vader, Length: 100, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['vader'] = tweets['tokenize_text'].apply(analyze_sentiment_vader)\n",
    "tweets['vader']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "062de0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "local_model_path = \"./bert_sst2_model\"\n",
    "tokenizer = BertTokenizer.from_pretrained(local_model_path)\n",
    "model = BertForSequenceClassification.from_pretrained(local_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d1f671ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     negative\n",
       "1     positive\n",
       "2     positive\n",
       "3     negative\n",
       "4     negative\n",
       "        ...   \n",
       "95    negative\n",
       "96    negative\n",
       "97    positive\n",
       "98    negative\n",
       "99    positive\n",
       "Name: bert, Length: 100, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_sentiment(tweet):\n",
    "    inputs = tokenizer(tweet, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    sentiment = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    return \"positive\" if sentiment == 1 else \"negative\"\n",
    "\n",
    "tweets['bert'] = tweets['clean_text'].apply(predict_sentiment)\n",
    "tweets['bert']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7d0005d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SentiWSP\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "local_model_path = \"./senti_WSP\"\n",
    "\n",
    "def analyze_sentiment_sentiwsp(tweet):\n",
    "    tokenizer = BertTokenizer.from_pretrained(local_model_path)\n",
    "    model = BertForSequenceClassification.from_pretrained(local_model_path)\n",
    "\n",
    "    inputs = tokenizer(tweet, return_tensors=\"pt\")\n",
    "    output = model(**inputs)\n",
    "    logits = output.logits\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    sentiment_class = torch.argmax(probabilities).item()\n",
    "\n",
    "    sentiment_mapping = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "    sentiment = sentiment_mapping[sentiment_class]\n",
    "\n",
    "    return sentiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70681bde",
   "metadata": {},
   "source": [
    "This function, analyze_sentiment_sentiwsp, takes a preprocessed tweet as input, tokenizes it using the SentiWSP tokenizer, and performs sentiment analysis using the pre-trained SentiWSP model from Hugging Face. The function returns the sentiment class (negative, neutral, or positive) based on the highest probability.\n",
    "\n",
    "In the example usage section, the function is applied to a sample preprocessed tweet, and the sentiment result from SentiWSP is printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "54c54af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      neutral\n",
       "1      neutral\n",
       "2     negative\n",
       "3     negative\n",
       "4     negative\n",
       "        ...   \n",
       "95     neutral\n",
       "96    negative\n",
       "97    negative\n",
       "98     neutral\n",
       "99     neutral\n",
       "Name: sentiwsp, Length: 100, dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['sentiwsp'] = tweets['clean_text'].apply(analyze_sentiment_sentiwsp)\n",
    "tweets['sentiwsp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e359e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
