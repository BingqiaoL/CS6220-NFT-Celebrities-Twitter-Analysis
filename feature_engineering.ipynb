{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1511"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mypath = '../../tweets_first_run'\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "len(onlyfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1829"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mypath = '../../tweets_second_run'\n",
    "onlyfiles2 = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "len(onlyfiles2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎| 1511/1511 [02:25<00:00, 10.35it/s]\n",
      " 14%|笆遺枕        | 263/1829 [00:56<03:59,  6.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".DS_Store\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎| 1829/1829 [12:01<00:00,  2.54it/s]\n"
     ]
    }
   ],
   "source": [
    "tweets = pd.DataFrame()\n",
    "\n",
    "for f in tqdm(onlyfiles):\n",
    "    try:\n",
    "        tweet = pd.read_csv('../../tweets_first_run/{}'.format(f))\n",
    "        tweets = tweets.append(tweet)\n",
    "    except:\n",
    "        print(f)\n",
    "\n",
    "for f in tqdm(onlyfiles2):\n",
    "    try:\n",
    "        tweet = pd.read_csv('../../tweets_second_run/{}'.format(f))\n",
    "        tweets = tweets.append(tweet)\n",
    "    except:\n",
    "        print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Username   User handle  \\\n",
      "0  Getgems TON NFT Marketplace  getgemsdotio   \n",
      "1  Getgems TON NFT Marketplace  getgemsdotio   \n",
      "2  Getgems TON NFT Marketplace  getgemsdotio   \n",
      "3  Getgems TON NFT Marketplace  getgemsdotio   \n",
      "4  Getgems TON NFT Marketplace  getgemsdotio   \n",
      "\n",
      "                                               Tweet  \\\n",
      "0  https://twitter.com/getgemsdotio/status/163025...   \n",
      "1  https://twitter.com/getgemsdotio/status/163022...   \n",
      "2  https://twitter.com/getgemsdotio/status/163022...   \n",
      "3  https://twitter.com/getgemsdotio/status/163021...   \n",
      "4  https://twitter.com/getgemsdotio/status/163021...   \n",
      "\n",
      "             Date of posting  \\\n",
      "0  2023-02-27 17:19:55+00:00   \n",
      "1  2023-02-27 15:21:22+00:00   \n",
      "2  2023-02-27 15:21:20+00:00   \n",
      "3  2023-02-27 14:35:23+00:00   \n",
      "4  2023-02-27 14:35:23+00:00   \n",
      "\n",
      "                                                Text Retweet count Like count  \\\n",
      "0            @Tleubayev @LostDogsCo Who? Who? Who? 操             0          2   \n",
      "1  More details about the new collection here筮ｸ十\n...             0          1   \n",
      "2  Remember we asked you to guess who's in the pi...             3          7   \n",
      "3  璃 https://t.co/frQpwixZ5s  @ston_fi had a prod...             0          2   \n",
      "4  璃 Fanzee @fanzeelabs token got listed on @Coin...             0          1   \n",
      "\n",
      "  Unnamed: 0  \n",
      "0        NaN  \n",
      "1        NaN  \n",
      "2        NaN  \n",
      "3        NaN  \n",
      "4        NaN  \n"
     ]
    }
   ],
   "source": [
    "print(tweets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_columns = ['Username', 'User handle', 'Tweet', 'Date of posting',\n",
    "       'Text', 'Retweet count', 'Like count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets[select_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Username</th>\n",
       "      <th>User handle</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Date of posting</th>\n",
       "      <th>Text</th>\n",
       "      <th>Retweet count</th>\n",
       "      <th>Like count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Getgems TON NFT Marketplace</td>\n",
       "      <td>getgemsdotio</td>\n",
       "      <td>https://twitter.com/getgemsdotio/status/163025...</td>\n",
       "      <td>2023-02-27 17:19:55+00:00</td>\n",
       "      <td>@Tleubayev @LostDogsCo Who? Who? Who? 操</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Getgems TON NFT Marketplace</td>\n",
       "      <td>getgemsdotio</td>\n",
       "      <td>https://twitter.com/getgemsdotio/status/163022...</td>\n",
       "      <td>2023-02-27 15:21:22+00:00</td>\n",
       "      <td>More details about the new collection here筮ｸ十\n...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Getgems TON NFT Marketplace</td>\n",
       "      <td>getgemsdotio</td>\n",
       "      <td>https://twitter.com/getgemsdotio/status/163022...</td>\n",
       "      <td>2023-02-27 15:21:20+00:00</td>\n",
       "      <td>Remember we asked you to guess who's in the pi...</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Getgems TON NFT Marketplace</td>\n",
       "      <td>getgemsdotio</td>\n",
       "      <td>https://twitter.com/getgemsdotio/status/163021...</td>\n",
       "      <td>2023-02-27 14:35:23+00:00</td>\n",
       "      <td>璃 https://t.co/frQpwixZ5s  @ston_fi had a prod...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Getgems TON NFT Marketplace</td>\n",
       "      <td>getgemsdotio</td>\n",
       "      <td>https://twitter.com/getgemsdotio/status/163021...</td>\n",
       "      <td>2023-02-27 14:35:23+00:00</td>\n",
       "      <td>璃 Fanzee @fanzeelabs token got listed on @Coin...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Username   User handle  \\\n",
       "0  Getgems TON NFT Marketplace  getgemsdotio   \n",
       "1  Getgems TON NFT Marketplace  getgemsdotio   \n",
       "2  Getgems TON NFT Marketplace  getgemsdotio   \n",
       "3  Getgems TON NFT Marketplace  getgemsdotio   \n",
       "4  Getgems TON NFT Marketplace  getgemsdotio   \n",
       "\n",
       "                                               Tweet  \\\n",
       "0  https://twitter.com/getgemsdotio/status/163025...   \n",
       "1  https://twitter.com/getgemsdotio/status/163022...   \n",
       "2  https://twitter.com/getgemsdotio/status/163022...   \n",
       "3  https://twitter.com/getgemsdotio/status/163021...   \n",
       "4  https://twitter.com/getgemsdotio/status/163021...   \n",
       "\n",
       "             Date of posting  \\\n",
       "0  2023-02-27 17:19:55+00:00   \n",
       "1  2023-02-27 15:21:22+00:00   \n",
       "2  2023-02-27 15:21:20+00:00   \n",
       "3  2023-02-27 14:35:23+00:00   \n",
       "4  2023-02-27 14:35:23+00:00   \n",
       "\n",
       "                                                Text Retweet count Like count  \n",
       "0            @Tleubayev @LostDogsCo Who? Who? Who? 操             0          2  \n",
       "1  More details about the new collection here筮ｸ十\n...             0          1  \n",
       "2  Remember we asked you to guess who's in the pi...             3          7  \n",
       "3  璃 https://t.co/frQpwixZ5s  @ston_fi had a prod...             0          2  \n",
       "4  璃 Fanzee @fanzeelabs token got listed on @Coin...             0          1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = tweets.drop_duplicates()\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5311125, 7)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write tweets to csv\n",
    "tweets.to_csv('all_tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/meganwang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 7)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv('all_tweets.csv')\n",
    "tweets = tweets.head(100)\n",
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean tweets\n",
    "def clean_tweet(tweet):\n",
    "    # Convert to string if not a string\n",
    "    tweet = str(tweet)\n",
    "    \n",
    "    # Remove URLs\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", tweet, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove user mentions\n",
    "    tweet = re.sub(r\"@\\w+\", \"\", tweet)\n",
    "\n",
    "    # Remove special characters\n",
    "    tweet = re.sub(r\"\\W\", \" \", tweet)\n",
    "\n",
    "    # Remove extra spaces\n",
    "    tweet = re.sub(r\"\\s+\", \" \", tweet)\n",
    "\n",
    "    return tweet.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               @Tleubayev @LostDogsCo Who? Who? Who? 操\n",
       "1     More details about the new collection here筮ｸ十\n...\n",
       "2     Remember we asked you to guess who's in the pi...\n",
       "3     璃 https://t.co/frQpwixZ5s  @ston_fi had a prod...\n",
       "4     璃 Fanzee @fanzeelabs token got listed on @Coin...\n",
       "                            ...                        \n",
       "95                          @0xBrigandine @loomdart lol\n",
       "96              @DanishCryptoDK @MORBS15 this is sick!､ｩ\n",
       "97                @Tleubayev you can't be too careful Ыn",
       "98    gm frens! now that Jan activity with eggs is f...\n",
       "99           @Tleubayev ｫ｡･ｰ thanks for the support, fam!\n",
       "Name: Text, Length: 100, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                           Who Who Who\n",
       "1            More details about the new collection here\n",
       "2     Remember we asked you to guess who s in the pi...\n",
       "3     had a product update The profitability of liqu...\n",
       "4     Fanzee token got listed on After joining the p...\n",
       "                            ...                        \n",
       "95                                                  lol\n",
       "96                                         this is sick\n",
       "97                             you can t be too careful\n",
       "98    gm frens now that Jan activity with eggs is fi...\n",
       "99                           thanks for the support fam\n",
       "Name: clean_text, Length: 100, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "tweets['clean_text'] = tweets['Text'].apply(clean_tweet)\n",
    "tweets['clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess tweets\n",
    "def preprocess_tweet(tweet):\n",
    "\n",
    "    # Tokenize the tweet\n",
    "    tokenizer = TweetTokenizer(preserve_case=False)\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatized_tokens = []\n",
    "    for token in tokens:\n",
    "        doc = nlp(token)\n",
    "        lemmatized_tokens.append([t.lemma_ for t in doc][0])\n",
    "\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in lemmatized_tokens]\n",
    "\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                    []\n",
       "1                                [detail, new, collect]\n",
       "2     [rememb, ask, guess, pictur, yep, adam, eve, d...\n",
       "3     [product, updat, profit, liquid, pool, import,...\n",
       "4     [fanze, token, get, list, join, project, team,...\n",
       "                            ...                        \n",
       "95                                                [lol]\n",
       "96                                               [sick]\n",
       "97                                               [care]\n",
       "98    [gm, fren, jan, activ, egg, finish, suggest, i...\n",
       "99                                [thank, support, fam]\n",
       "Name: tokenize_text, Length: 100, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['tokenize_text'] = tweets['clean_text'].apply(preprocess_tweet)\n",
    "tweets['tokenize_text']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Bert to get embeddings for each cleaned tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read tweets from all_tweets.csv\n",
    "tweets = pd.read_csv('all_tweets.csv')\n",
    "tweets = tweets.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply clean_tweet function\n",
    "tweets['clean_text'] = tweets['Text'].apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Use bert to get embeddings for each cleaned tweet\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# load the pre-trained BERT model.\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "model = AutoModel.from_pretrained('bert-base-cased')\n",
    "# tweets = pd.read_csv('100_tweets_with_bert_embeddings.csv')\n",
    "# tweets = tweets.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                           Who Who Who\n",
       "1            More details about the new collection here\n",
       "2     Remember we asked you to guess who s in the pi...\n",
       "3     had a product update The profitability of liqu...\n",
       "4     Fanzee token got listed on After joining the p...\n",
       "                            ...                        \n",
       "95                                                  lol\n",
       "96                                         this is sick\n",
       "97                             you can t be too careful\n",
       "98    gm frens now that Jan activity with eggs is fi...\n",
       "99                           thanks for the support fam\n",
       "Name: clean_text, Length: 100, dtype: object"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print tweets['clean_text']\n",
    "tweets['clean_text'][0]\n",
    "tweets['clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the embeddings\n",
    "embeddings = []\n",
    "\n",
    "# Iterate through the 'clean_text' column\n",
    "for tweet in tweets['clean_text']:\n",
    "    # Tokenize the tweet\n",
    "    tokens = tokenizer.encode(tweet, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Pass the tokens through the BERT model to obtain the embeddings\n",
    "    with torch.no_grad():\n",
    "        output = model(tokens)[0]\n",
    "\n",
    "        # Use the pooler_output tensor to obtain a fixed-sized representation of the entire input sequence\n",
    "        sentence_embedding = output[:, 0, :].squeeze().numpy()\n",
    "        embeddings.append(sentence_embedding)\n",
    "\n",
    "# Add the sentence embeddings to the 'tweets' DataFrame\n",
    "tweets['mean_bert_embeddings'] = embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [0.28860363, -0.09343399, 0.10638529, -0.05212...\n",
       "1     [0.5219427, 0.043911606, 0.2320774, -0.4046072...\n",
       "2     [0.579924, 0.09792202, 0.11067101, -0.2658087,...\n",
       "3     [0.45673087, 0.17961024, -0.06802698, -0.36705...\n",
       "4     [0.4253203, 0.09439247, -0.3472338, -0.3259493...\n",
       "                            ...                        \n",
       "95    [0.46448538, 0.13541013, 0.30550066, -0.310473...\n",
       "96    [0.44673324, 0.17393774, 0.14376909, -0.383022...\n",
       "97    [0.40385568, 0.1397439, 0.03351564, -0.2454058...\n",
       "98    [0.50105846, 0.041419126, 0.12326328, -0.15445...\n",
       "99    [-0.029451327, 0.038624473, -0.09136265, -0.46...\n",
       "Name: mean_bert_embeddings, Length: 100, dtype: object"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print first five tweet bert embeddings\n",
    "tweets['mean_bert_embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print dimension of each bert embedding\n",
    "tweets['mean_bert_embeddings'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Username</th>\n",
       "      <th>User handle</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Date of posting</th>\n",
       "      <th>Text</th>\n",
       "      <th>Retweet count</th>\n",
       "      <th>Like count</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>mean_bert_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Getgems TON NFT Marketplace</td>\n",
       "      <td>getgemsdotio</td>\n",
       "      <td>https://twitter.com/getgemsdotio/status/163025...</td>\n",
       "      <td>2023-02-27 17:19:55+00:00</td>\n",
       "      <td>@Tleubayev @LostDogsCo Who? Who? Who? 操</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Who Who Who</td>\n",
       "      <td>[0.28860363, -0.09343399, 0.10638529, -0.05212...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Getgems TON NFT Marketplace</td>\n",
       "      <td>getgemsdotio</td>\n",
       "      <td>https://twitter.com/getgemsdotio/status/163022...</td>\n",
       "      <td>2023-02-27 15:21:22+00:00</td>\n",
       "      <td>More details about the new collection here筮ｸ十\n...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>More details about the new collection here</td>\n",
       "      <td>[0.5219427, 0.043911606, 0.2320774, -0.4046072...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Getgems TON NFT Marketplace</td>\n",
       "      <td>getgemsdotio</td>\n",
       "      <td>https://twitter.com/getgemsdotio/status/163022...</td>\n",
       "      <td>2023-02-27 15:21:20+00:00</td>\n",
       "      <td>Remember we asked you to guess who's in the pi...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Remember we asked you to guess who s in the pi...</td>\n",
       "      <td>[0.579924, 0.09792202, 0.11067101, -0.2658087,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Getgems TON NFT Marketplace</td>\n",
       "      <td>getgemsdotio</td>\n",
       "      <td>https://twitter.com/getgemsdotio/status/163021...</td>\n",
       "      <td>2023-02-27 14:35:23+00:00</td>\n",
       "      <td>璃 https://t.co/frQpwixZ5s  @ston_fi had a prod...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>had a product update The profitability of liqu...</td>\n",
       "      <td>[0.45673087, 0.17961024, -0.06802698, -0.36705...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Getgems TON NFT Marketplace</td>\n",
       "      <td>getgemsdotio</td>\n",
       "      <td>https://twitter.com/getgemsdotio/status/163021...</td>\n",
       "      <td>2023-02-27 14:35:23+00:00</td>\n",
       "      <td>璃 Fanzee @fanzeelabs token got listed on @Coin...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Fanzee token got listed on After joining the p...</td>\n",
       "      <td>[0.4253203, 0.09439247, -0.3472338, -0.3259493...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Username   User handle  \\\n",
       "0  Getgems TON NFT Marketplace  getgemsdotio   \n",
       "1  Getgems TON NFT Marketplace  getgemsdotio   \n",
       "2  Getgems TON NFT Marketplace  getgemsdotio   \n",
       "3  Getgems TON NFT Marketplace  getgemsdotio   \n",
       "4  Getgems TON NFT Marketplace  getgemsdotio   \n",
       "\n",
       "                                               Tweet  \\\n",
       "0  https://twitter.com/getgemsdotio/status/163025...   \n",
       "1  https://twitter.com/getgemsdotio/status/163022...   \n",
       "2  https://twitter.com/getgemsdotio/status/163022...   \n",
       "3  https://twitter.com/getgemsdotio/status/163021...   \n",
       "4  https://twitter.com/getgemsdotio/status/163021...   \n",
       "\n",
       "             Date of posting  \\\n",
       "0  2023-02-27 17:19:55+00:00   \n",
       "1  2023-02-27 15:21:22+00:00   \n",
       "2  2023-02-27 15:21:20+00:00   \n",
       "3  2023-02-27 14:35:23+00:00   \n",
       "4  2023-02-27 14:35:23+00:00   \n",
       "\n",
       "                                                Text  Retweet count  \\\n",
       "0            @Tleubayev @LostDogsCo Who? Who? Who? 操            0.0   \n",
       "1  More details about the new collection here筮ｸ十\n...            0.0   \n",
       "2  Remember we asked you to guess who's in the pi...            3.0   \n",
       "3  璃 https://t.co/frQpwixZ5s  @ston_fi had a prod...            0.0   \n",
       "4  璃 Fanzee @fanzeelabs token got listed on @Coin...            0.0   \n",
       "\n",
       "   Like count                                         clean_text  \\\n",
       "0         2.0                                        Who Who Who   \n",
       "1         1.0         More details about the new collection here   \n",
       "2         7.0  Remember we asked you to guess who s in the pi...   \n",
       "3         2.0  had a product update The profitability of liqu...   \n",
       "4         1.0  Fanzee token got listed on After joining the p...   \n",
       "\n",
       "                                mean_bert_embeddings  \n",
       "0  [0.28860363, -0.09343399, 0.10638529, -0.05212...  \n",
       "1  [0.5219427, 0.043911606, 0.2320774, -0.4046072...  \n",
       "2  [0.579924, 0.09792202, 0.11067101, -0.2658087,...  \n",
       "3  [0.45673087, 0.17961024, -0.06802698, -0.36705...  \n",
       "4  [0.4253203, 0.09439247, -0.3472338, -0.3259493...  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select_columns = ['Username', 'User handle', 'Tweet', 'Date of posting', 'Text', 'Retweet count', 'Like count', 'clean_text', 'tokenize_text', 'bert_embeddings']\n",
    "\n",
    "select_columns = ['Username', 'User handle', 'Tweet', 'Date of posting', 'Text', 'Retweet count', 'Like count', 'clean_text', 'mean_bert_embeddings']\n",
    "\n",
    "tweets = tweets[select_columns]\n",
    "\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store tweets to csv\n",
    "tweets.to_csv('100_tweets_with_mean_bert_embeddings.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTweet to get embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load BERTweet tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\")\n",
    "model = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "\n",
    "# Initialize a list to store the embeddings\n",
    "embeddings = []\n",
    "\n",
    "# Iterate through the 'clean_text' column\n",
    "for tweet in tweets['clean_text']:\n",
    "    # Tokenize the tweet\n",
    "    tokens = tokenizer.encode(tweet, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Pass the tokens through the BERTweet model to obtain the embeddings\n",
    "    with torch.no_grad():\n",
    "        output = model(tokens)[0]\n",
    "\n",
    "    # Calculate the mean of the token embeddings\n",
    "    token_embeddings = output.squeeze().numpy()\n",
    "    mean_tweet_embedding = np.mean(token_embeddings, axis=0)\n",
    "    embeddings.append(mean_tweet_embedding)\n",
    "\n",
    "# Add the mean tweet embeddings to the 'tweets' DataFrame\n",
    "tweets['mean_bertweet_embeddings'] = embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [-0.1800077, 0.1929334, 0.035471354, 0.1299720...\n",
       "1     [-0.20074762, -0.038995907, 0.11312827, 0.2415...\n",
       "2     [-0.102422714, -0.08263994, 0.02710068, 0.0452...\n",
       "3     [-0.17674993, 0.029806131, 0.18233073, 0.05352...\n",
       "4     [-0.14970917, -0.06618342, 0.09273748, -0.0180...\n",
       "                            ...                        \n",
       "95    [-0.3540177, 0.052014723, 0.14879872, 0.028366...\n",
       "96    [-0.0928275, 0.1617451, 0.20246832, -0.0465133...\n",
       "97    [-0.05603843, -0.019335262, 0.17461726, 0.2828...\n",
       "98    [-0.003259917, -0.09178815, 0.107570715, 0.094...\n",
       "99    [0.15878811, 0.06531352, 0.24042082, 0.1343150...\n",
       "Name: mean_bertweet_embeddings, Length: 100, dtype: object"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print first five tweet bertweet embeddings\n",
    "tweets['mean_bertweet_embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store tweets to csv\n",
    "tweets.to_csv('100_tweets_with_mean_bertweet_embeddings.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XLNet to get tweet embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetModel: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import XLNetTokenizer, XLNetModel\n",
    "\n",
    "# Load XLNet tokenizer and model\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "model = XLNetModel.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "# ... (rest of the code remains the same)\n",
    "\n",
    "\n",
    "# Initialize a list to store the embeddings\n",
    "embeddings = []\n",
    "\n",
    "# Iterate through the 'clean_text' column\n",
    "for tweet in tweets['clean_text']:\n",
    "    # Tokenize the tweet\n",
    "    tokens = tokenizer.encode(tweet, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Pass the tokens through the XLNet model to obtain the embeddings\n",
    "    with torch.no_grad():\n",
    "        output = model(tokens)[0]\n",
    "\n",
    "    # Calculate the mean of the token embeddings\n",
    "    token_embeddings = output.squeeze().numpy()\n",
    "    mean_tweet_embedding = np.mean(token_embeddings, axis=0)\n",
    "    embeddings.append(mean_tweet_embedding)\n",
    "\n",
    "# Add the mean tweet embeddings to the 'tweets' DataFrame\n",
    "tweets['mean_xlnet_embeddings'] = embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [1.2993158, -0.16114616, -0.9166239, -0.172533...\n",
       "1     [2.0206137, -0.9051688, -2.5756865, -0.8674592...\n",
       "2     [-0.28351945, -0.2873172, -1.4390882, 1.163171...\n",
       "3     [0.99469995, -0.92810184, 0.46260703, 0.323182...\n",
       "4     [-0.7802155, -1.7383583, -1.0635799, 2.0455844...\n",
       "                            ...                        \n",
       "95    [2.1280346, -1.3491827, -3.19979, -0.13633314,...\n",
       "96    [2.6570697, -1.4308031, -1.9725819, 2.0327384,...\n",
       "97    [2.6872575, -0.06791557, -3.6689487, 2.8445234...\n",
       "98    [-0.002103882, 0.36363763, -0.64697796, 0.8165...\n",
       "99    [1.2170751, -0.7641735, -3.052117, 0.977632, -...\n",
       "Name: mean_xlnet_embeddings, Length: 100, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print first five tweet xlnet embeddings\n",
    "tweets['mean_xlnet_embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print each embedding dimension\n",
    "tweets['mean_xlnet_embeddings'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store tweets to csv\n",
    "tweets.to_csv('100_tweets_with_mean_xlnet_embeddings.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for each tweet embedding, concatenate retweet count and like count to generate new embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Username</th>\n",
       "      <th>User handle</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Date of posting</th>\n",
       "      <th>Text</th>\n",
       "      <th>Retweet count</th>\n",
       "      <th>Like count</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>mean_bert_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Getgems TON NFT Marketplace</td>\n",
       "      <td>getgemsdotio</td>\n",
       "      <td>https://twitter.com/getgemsdotio/status/163025...</td>\n",
       "      <td>2023-02-27 17:19:55+00:00</td>\n",
       "      <td>@Tleubayev @LostDogsCo Who? Who? Who? 操</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Who Who Who</td>\n",
       "      <td>[ 2.88603634e-01 -9.34339911e-02  1.06385291e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Getgems TON NFT Marketplace</td>\n",
       "      <td>getgemsdotio</td>\n",
       "      <td>https://twitter.com/getgemsdotio/status/163022...</td>\n",
       "      <td>2023-02-27 15:21:22+00:00</td>\n",
       "      <td>More details about the new collection here筮ｸ十\n...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>More details about the new collection here</td>\n",
       "      <td>[ 5.21942675e-01  4.39116061e-02  2.32077405e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Getgems TON NFT Marketplace</td>\n",
       "      <td>getgemsdotio</td>\n",
       "      <td>https://twitter.com/getgemsdotio/status/163022...</td>\n",
       "      <td>2023-02-27 15:21:20+00:00</td>\n",
       "      <td>Remember we asked you to guess who's in the pi...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Remember we asked you to guess who s in the pi...</td>\n",
       "      <td>[ 5.79923987e-01  9.79220197e-02  1.10671014e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Getgems TON NFT Marketplace</td>\n",
       "      <td>getgemsdotio</td>\n",
       "      <td>https://twitter.com/getgemsdotio/status/163021...</td>\n",
       "      <td>2023-02-27 14:35:23+00:00</td>\n",
       "      <td>璃 https://t.co/frQpwixZ5s  @ston_fi had a prod...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>had a product update The profitability of liqu...</td>\n",
       "      <td>[ 4.56730872e-01  1.79610237e-01 -6.80269822e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Getgems TON NFT Marketplace</td>\n",
       "      <td>getgemsdotio</td>\n",
       "      <td>https://twitter.com/getgemsdotio/status/163021...</td>\n",
       "      <td>2023-02-27 14:35:23+00:00</td>\n",
       "      <td>璃 Fanzee @fanzeelabs token got listed on @Coin...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Fanzee token got listed on After joining the p...</td>\n",
       "      <td>[ 4.25320297e-01  9.43924710e-02 -3.47233802e-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Username   User handle  \\\n",
       "0  Getgems TON NFT Marketplace  getgemsdotio   \n",
       "1  Getgems TON NFT Marketplace  getgemsdotio   \n",
       "2  Getgems TON NFT Marketplace  getgemsdotio   \n",
       "3  Getgems TON NFT Marketplace  getgemsdotio   \n",
       "4  Getgems TON NFT Marketplace  getgemsdotio   \n",
       "\n",
       "                                               Tweet  \\\n",
       "0  https://twitter.com/getgemsdotio/status/163025...   \n",
       "1  https://twitter.com/getgemsdotio/status/163022...   \n",
       "2  https://twitter.com/getgemsdotio/status/163022...   \n",
       "3  https://twitter.com/getgemsdotio/status/163021...   \n",
       "4  https://twitter.com/getgemsdotio/status/163021...   \n",
       "\n",
       "             Date of posting  \\\n",
       "0  2023-02-27 17:19:55+00:00   \n",
       "1  2023-02-27 15:21:22+00:00   \n",
       "2  2023-02-27 15:21:20+00:00   \n",
       "3  2023-02-27 14:35:23+00:00   \n",
       "4  2023-02-27 14:35:23+00:00   \n",
       "\n",
       "                                                Text  Retweet count  \\\n",
       "0            @Tleubayev @LostDogsCo Who? Who? Who? 操            0.0   \n",
       "1  More details about the new collection here筮ｸ十\n...            0.0   \n",
       "2  Remember we asked you to guess who's in the pi...            3.0   \n",
       "3  璃 https://t.co/frQpwixZ5s  @ston_fi had a prod...            0.0   \n",
       "4  璃 Fanzee @fanzeelabs token got listed on @Coin...            0.0   \n",
       "\n",
       "   Like count                                         clean_text  \\\n",
       "0         2.0                                        Who Who Who   \n",
       "1         1.0         More details about the new collection here   \n",
       "2         7.0  Remember we asked you to guess who s in the pi...   \n",
       "3         2.0  had a product update The profitability of liqu...   \n",
       "4         1.0  Fanzee token got listed on After joining the p...   \n",
       "\n",
       "                                mean_bert_embeddings  \n",
       "0  [ 2.88603634e-01 -9.34339911e-02  1.06385291e-...  \n",
       "1  [ 5.21942675e-01  4.39116061e-02  2.32077405e-...  \n",
       "2  [ 5.79923987e-01  9.79220197e-02  1.10671014e-...  \n",
       "3  [ 4.56730872e-01  1.79610237e-01 -6.80269822e-...  \n",
       "4  [ 4.25320297e-01  9.43924710e-02 -3.47233802e-...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read 100_tweets_with_mean_bert_embeddings.csv\n",
    "tweets = pd.read_csv('100_tweets_with_mean_bert_embeddings.csv')\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new column: features, it is the concatenation of mean_bert_embeddings, and retweet count and like count\n",
    "tweets['features'] = tweets['mean_bert_embeddings'].astype(str) + tweets['Retweet count'].astype(str) + tweets['Like count'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print type of tweets['mean_bert_embeddings'][0]\n",
    "type(tweets['mean_bert_embeddings'][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
